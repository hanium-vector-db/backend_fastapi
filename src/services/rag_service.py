from langchain.schema.runnable import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
import logging
import os
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.helpers import search_news, create_documents_from_news, search_latest_news, get_news_summary_with_tavily
from utils.config_loader import config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- ì„¤ì •ì—ì„œ ì½ì–´ì˜¨ ìƒìˆ˜ë“¤ ---
DB_PERSIST_DIRECTORY = config.vector_db_path

class RAGService:
    def __init__(self, llm_handler, embedding_handler):
        self.llm_handler = llm_handler
        self.embedding_handler = embedding_handler
        self.db = None
        self.retriever = None
        self.rag_chain = None
        self._initialize_rag()

    def _initialize_rag(self):
        try:
            logger.info("Initializing RAG service...")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not os.path.exists(DB_PERSIST_DIRECTORY):
                os.makedirs(DB_PERSIST_DIRECTORY)
                logger.info(f"Created vector database directory: {DB_PERSIST_DIRECTORY}")

            # ê¸°ì¡´ DBë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±
            self.db = Chroma(
                persist_directory=DB_PERSIST_DIRECTORY,
                embedding_function=self.embedding_handler.embeddings,
                collection_metadata={'hnsw:space': 'l2'}
            )
            
            logger.info(f"Vector database loaded/initialized from: {DB_PERSIST_DIRECTORY}")
            logger.info(f"Current document count: {self.db._collection.count()}")

            # Create retriever
            self.retriever = self.db.as_retriever(search_kwargs={'k': 3})

            # Setup RAG chain
            self._setup_rag_chain()
            
            logger.info("RAG service initialized successfully!")
            
        except Exception as e:
            logger.error(f"Error initializing RAG service: {e}")
            raise

    def add_documents_from_web(self, query: str, max_results: int = 5):
        """
        ì›¹ì—ì„œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  í•´ë‹¹ ë‚´ìš©ì„ Vector DBì— ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        try:
            logger.info(f"Starting to add documents from web for query: '{query}'")
            # 1. ë‰´ìŠ¤ ê²€ìƒ‰
            news_results = search_news(query, max_results)
            if not news_results:
                return 0, "No news articles found."

            # 2. ë¬¸ì„œ ìƒì„± (ìŠ¤í¬ë˜í•‘)
            documents = create_documents_from_news(news_results)
            if not documents:
                return 0, "Failed to create documents from news articles."

            # 3. í…ìŠ¤íŠ¸ ë¶„í•  (ì„¤ì •ì—ì„œ ì½ì–´ì˜´)
            external_web_config = config.external_web_rag_config
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=external_web_config['chunk_size'], 
                chunk_overlap=external_web_config['chunk_overlap']
            )
            chunks = text_splitter.split_documents(documents)
            logger.info(f"Created {len(chunks)} chunks from {len(documents)} documents.")

            # 4. DBì— ì¶”ê°€ (persist ì—ëŸ¬ ë°©ì§€)
            if chunks:
                try:
                    self.db.add_documents(chunks)
                    logger.info(f"Successfully added {len(chunks)} new chunks to the vector database.")
                    return len(chunks), f"Successfully added {len(chunks)} new chunks to the database."
                except Exception as persist_error:
                    logger.warning(f"Persist error (ignoring): {persist_error}")
                    # persist ì‹¤íŒ¨í•´ë„ ë©”ëª¨ë¦¬ì—ëŠ” ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                    return len(chunks), f"Successfully added {len(chunks)} new chunks to the database (in-memory)."
            else:
                return 0, "No processable content found in the articles."
        except Exception as e:
            logger.error(f"Error adding documents from web: {e}")
            return 0, f"An error occurred: {str(e)}"

    def _setup_rag_chain(self):
        try:
            # RAG prompt
            rag_prompt = ChatPromptTemplate.from_messages([
                ('system', 'ë‹¤ìŒ Contextë¥¼ ì‚¬ìš©í•˜ì—¬ Questionì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë§Œì•½ Contextì— ì •ë³´ê°€ ì—†ë‹¤ë©´, ì•„ëŠ”ëŒ€ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.'),
                ('user', 'Context: {context}\n---\nQuestion: {question}')
            ])

            # Create the RAG chain
            self.rag_chain = (
                {"context": self.retriever | self.format_docs, "question": RunnablePassthrough()}
                | rag_prompt
                | self.llm_handler.chat_model
                | StrOutputParser()
            )
            
        except Exception as e:
            logger.error(f"Error setting up RAG chain: {e}")
            raise

    def format_docs(self, docs):
        return "\n---\n".join(f"ì¶œì²˜: {doc.metadata.get('source', 'Unknown')}\nì œëª©: {doc.metadata.get('title', 'Unknown')}\në‚´ìš©: {doc.page_content}" for doc in docs)

    def generate_response(self, query: str) -> str:
        try:
            if self.rag_chain is None:
                return "RAG service not initialized"
            
            response = self.rag_chain.invoke(query)
            
            # Handle different response types
            if hasattr(response, 'content'):
                return response.content
            elif isinstance(response, str):
                return response
            else:
                return str(response)
            
        except Exception as e:
            logger.error(f"Error generating RAG response: {e}")
            return f"Error: {str(e)}"

    def get_relevant_documents(self, query: str, k: int = 3):
        try:
            if self.retriever is None:
                return []
            
            docs = self.retriever.invoke(query)
            return [{"title": doc.metadata.get('title', 'Unknown'), "content": doc.page_content, "source": doc.metadata.get('source', 'Unknown')}
 for doc in docs]
            
        except Exception as e:
            logger.error(f"Error retrieving documents: {e}")
            return []

    # === ìƒˆë¡œìš´ ë‰´ìŠ¤ ê´€ë ¨ ê¸°ëŠ¥ë“¤ ===
    
    def get_latest_news(self, categories: list = None, max_results: int = 10, time_range: str = 'd'):
        """
        ìµœì‹  ë‰´ìŠ¤ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ì§€ ì•Šê³  ì¡°íšŒë§Œ)
        """
        try:
            logger.info(f"ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ ì¤‘... ì¹´í…Œê³ ë¦¬: {categories}, ê²°ê³¼ìˆ˜: {max_results}")
            news_results = search_latest_news(max_results=max_results, categories=categories, time_range=time_range)
            
            formatted_news = []
            for news in news_results:
                formatted_news.append({
                    'title': news.get('title', ''),
                    'url': news.get('url', ''),
                    'content': news.get('content', ''),
                    'category': news.get('category', 'general'),
                    'published_date': news.get('published_date', ''),
                    'score': news.get('score', 0)
                })
            
            return formatted_news
            
        except Exception as e:
            logger.error(f"ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def summarize_news(self, query: str, max_results: int = 5, summary_type: str = "comprehensive"):
        """
        íŠ¹ì • ì£¼ì œì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  LLMì„ ì´ìš©í•´ ìš”ì•½í•©ë‹ˆë‹¤
        
        Args:
            query: ê²€ìƒ‰í•  ì£¼ì œ
            max_results: ë¶„ì„í•  ë‰´ìŠ¤ ê°œìˆ˜
            summary_type: ìš”ì•½ íƒ€ì… ("brief", "comprehensive", "analysis")
        """
        try:
            logger.info(f"'{query}' ì£¼ì œ ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì¤‘...")
            
            # Tavilyë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ê¸°ë³¸ ìš”ì•½ íšë“
            news_data = get_news_summary_with_tavily(query, max_results)
            
            if not news_data:
                return {
                    "summary": f"'{query}' ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "articles": [],
                    "keywords": [],
                    "sentiment": "neutral"
                }
            
            # ìš”ì•½ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            summary_prompts = {
                "brief": self._get_brief_summary_prompt(),
                "comprehensive": self._get_comprehensive_summary_prompt(),
                "analysis": self._get_analysis_summary_prompt()
            }
            
            prompt_template = summary_prompts.get(summary_type, summary_prompts["comprehensive"])
            
            # ë‰´ìŠ¤ ë°ì´í„° ì¤€ë¹„
            articles_text = "\n\n".join([
                f"ì œëª©: {article.get('title', '')}\në‚´ìš©: {article.get('content', '')[:1000]}"
                for article in news_data[:max_results]
                if not article.get('is_summary', False)  # Tavilyì˜ ìë™ ìš”ì•½ ì œì™¸
            ])
            
            # LLMìœ¼ë¡œ ìš”ì•½ ìƒì„±
            full_prompt = prompt_template.format(query=query, articles=articles_text)
            summary_response = self.llm_handler.chat_model.invoke(full_prompt)
            
            return {
                "summary": summary_response.content if hasattr(summary_response, 'content') else str(summary_response),
                "articles": news_data[:max_results],
                "query": query,
                "summary_type": summary_type,
                "total_articles": len(news_data)
            }
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                "summary": f"ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "articles": [],
                "keywords": [],
                "sentiment": "neutral"
            }
    
    def analyze_news_trends(self, categories: list = None, max_results: int = 20, time_range: str = 'd'):
        """
        ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ íŠ¸ë Œë“œë¥¼ íŒŒì•…í•©ë‹ˆë‹¤
        """
        try:
            logger.info(f"ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘... ì¹´í…Œê³ ë¦¬: {categories}")
            
            if categories is None:
                categories = ['politics', 'economy', 'technology', 'society']
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
            all_news = []
            category_summaries = {}
            
            for category in categories:
                category_news = search_news(
                    "ìµœì‹  ë‰´ìŠ¤", 
                    max_results=max_results//len(categories), 
                    category=category,
                    time_range=time_range
                )
                
                if category_news:
                    all_news.extend(category_news)
                    
                    # ì¹´í…Œê³ ë¦¬ë³„ ê°„ë‹¨ ìš”ì•½
                    category_text = "\n".join([
                        f"â€¢ {news.get('title', '')}: {news.get('content', '')[:200]}"
                        for news in category_news[:3]
                    ])
                    
                    category_prompt = f"ë‹¤ìŒ {category} ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ë“¤ì˜ ì£¼ìš” íŠ¸ë Œë“œë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n{category_text}"
                    category_summary = self.llm_handler.chat_model.invoke(category_prompt)
                    category_summaries[category] = category_summary.content if hasattr(category_summary, 'content') else str(category_summary)
            
            # ì „ì²´ íŠ¸ë Œë“œ ë¶„ì„
            trend_analysis_prompt = self._get_trend_analysis_prompt()
            all_titles = [news.get('title', '') for news in all_news]
            titles_text = "\n".join([f"â€¢ {title}" for title in all_titles[:30]])
            
            full_trend_prompt = trend_analysis_prompt.format(
                titles=titles_text,
                category_summaries="\n".join([f"{cat}: {summary}" for cat, summary in category_summaries.items()])
            )
            
            trend_response = self.llm_handler.chat_model.invoke(full_trend_prompt)
            
            return {
                "overall_trend": trend_response.content if hasattr(trend_response, 'content') else str(trend_response),
                "category_trends": category_summaries,
                "total_articles_analyzed": len(all_news),
                "categories": categories,
                "time_range": time_range
            }
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "overall_trend": f"íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "category_trends": {},
                "total_articles_analyzed": 0
            }

    # === í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ë“¤ ===
    
    def _get_brief_summary_prompt(self):
        """ê°„ë‹¨ ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸"""
        return """ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{query}' ì£¼ì œì— ëŒ€í•œ ê°„ë‹¨í•œ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ê¸°ì‚¬ë“¤:
{articles}

ìš”êµ¬ì‚¬í•­:
1. í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ìš”ì•½
2. ê°€ì¥ ì¤‘ìš”í•œ í¬ì¸íŠ¸ë§Œ í¬í•¨
3. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±
4. í•œêµ­ì–´ë¡œ ì‘ì„±

ê°„ë‹¨ ìš”ì•½:"""

    def _get_comprehensive_summary_prompt(self):
        """í¬ê´„ì  ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸"""
        return """ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{query}' ì£¼ì œì— ëŒ€í•œ í¬ê´„ì ì¸ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ê¸°ì‚¬ë“¤:
{articles}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸ“° ì£¼ìš” ë‚´ìš© ìš”ì•½
(í•µì‹¬ ë‚´ìš©ì„ 3-4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½)

## ğŸ” ì„¸ë¶€ ë¶„ì„
â€¢ ì£¼ìš” ì´ìŠˆ: 
â€¢ ê´€ë ¨ ì¸ë¬¼/ê¸°ê´€:
â€¢ ì˜í–¥/ê²°ê³¼:

## ğŸ·ï¸ í‚¤ì›Œë“œ
(ê´€ë ¨ í‚¤ì›Œë“œ 3-5ê°œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„)

## ğŸ“Š ì¢…í•© í‰ê°€
(ì „ë°˜ì ì¸ ìƒí™© í‰ê°€ì™€ í–¥í›„ ì „ë§ 1-2ë¬¸ì¥)

ëª¨ë“  ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    def _get_analysis_summary_prompt(self):
        """ë¶„ì„ ì¤‘ì‹¬ ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸"""
        return """ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{query}' ì£¼ì œì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ê¸°ì‚¬ë“¤:
{articles}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

## ğŸ¯ í•µì‹¬ ì´ìŠˆ ë¶„ì„
(ê°€ì¥ ì¤‘ìš”í•œ ì´ìŠˆì™€ ê·¸ ë°°ê²½)

## ğŸ“ˆ í˜„í™© ë° íŠ¸ë Œë“œ
â€¢ í˜„ì¬ ìƒí™©:
â€¢ ë³€í™” ì¶”ì´:
â€¢ ì£¼ëª©í•  ì :

## âš¡ ì£¼ìš” ë™í–¥
â€¢ ê¸ì •ì  ìš”ì†Œ:
â€¢ ìš°ë ¤ì‚¬í•­:
â€¢ ì˜ˆìƒ ì‹œë‚˜ë¦¬ì˜¤:

## ğŸŒŸ ì‹œì‚¬ì  ë° ì „ë§
(ì´ ë‰´ìŠ¤ê°€ ê°–ëŠ” ì˜ë¯¸ì™€ í–¥í›„ ì˜ˆìƒë˜ëŠ” ë°œì „ ë°©í–¥)

## ğŸ·ï¸ í•µì‹¬ í‚¤ì›Œë“œ
(ë¶„ì„ì— ì¤‘ìš”í•œ í‚¤ì›Œë“œ 5-7ê°œ)

ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ ì‹œê°ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    def _get_trend_analysis_prompt(self):
        """íŠ¸ë Œë“œ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸"""
        return """ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ë“¤ê³¼ ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ì œëª©ë“¤:
{titles}

ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½:
{category_summaries}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

## ğŸ”¥ ì˜¤ëŠ˜ì˜ ì£¼ìš” íŠ¸ë Œë“œ
(ê°€ì¥ ì£¼ëª©ë°›ëŠ” ì´ìŠˆ 2-3ê°œ)

## ğŸ“Š ë¶„ì•¼ë³„ ë™í–¥
â€¢ ì •ì¹˜: (ì •ì¹˜ ê´€ë ¨ ì£¼ìš” ì´ìŠˆ)
â€¢ ê²½ì œ: (ê²½ì œ ê´€ë ¨ ì£¼ìš” ì´ìŠˆ)  
â€¢ ì‚¬íšŒ: (ì‚¬íšŒ ê´€ë ¨ ì£¼ìš” ì´ìŠˆ)
â€¢ ê¸°ìˆ : (ê¸°ìˆ  ê´€ë ¨ ì£¼ìš” ì´ìŠˆ)

## ğŸ­ ì—¬ë¡  ë° ê´€ì‹¬ë„
(êµ­ë¯¼ë“¤ì´ ê°€ì¥ ê´€ì‹¬ ê°–ëŠ” ì´ìŠˆë“¤ê³¼ ì—¬ë¡ ì˜ ë°©í–¥)

## ğŸ”® ì£¼ëª©í•  í¬ì¸íŠ¸
(ì•ìœ¼ë¡œ ê³„ì† ì£¼ëª©í•´ì•¼ í•  ì´ìŠˆë“¤)

ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ì‹œê°ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
