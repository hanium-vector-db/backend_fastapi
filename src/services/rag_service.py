from langchain.schema.runnable import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
import logging
import os
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.helpers import search_news, create_documents_from_news, search_latest_news, get_news_summary_with_tavily
from utils.config_loader import config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- ì„¤ì •ì—ì„œ ì½ì–´ì˜¨ ìƒìˆ˜ë“¤ ---
DB_PERSIST_DIRECTORY = config.vector_db_path

class RAGService:
    def __init__(self, llm_handler, embedding_handler):
        self.llm_handler = llm_handler
        self.embedding_handler = embedding_handler
        self.db = None
        self.retriever = None
        self.rag_chain = None
        self._initialize_rag()

    def _initialize_rag(self):
        try:
            logger.info("Initializing RAG service...")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not os.path.exists(DB_PERSIST_DIRECTORY):
                os.makedirs(DB_PERSIST_DIRECTORY)
                logger.info(f"Created vector database directory: {DB_PERSIST_DIRECTORY}")

            # ê¸°ì¡´ DBë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±
            self.db = Chroma(
                persist_directory=DB_PERSIST_DIRECTORY,
                embedding_function=self.embedding_handler.embeddings,
                collection_metadata={'hnsw:space': 'l2'}
            )
            
            logger.info(f"Vector database loaded/initialized from: {DB_PERSIST_DIRECTORY}")
            logger.info(f"Current document count: {self.db._collection.count()}")

            # Create retriever - ë” ë§ì€ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            self.retriever = self.db.as_retriever(search_kwargs={'k': 8})

            # Setup RAG chain
            self._setup_rag_chain()
            
            logger.info("RAG service initialized successfully!")
            
        except Exception as e:
            logger.error(f"Error initializing RAG service: {e}")
            raise

    def add_documents_from_web(self, query: str, max_results: int = 5):
        """
        ì›¹ì—ì„œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  í•´ë‹¹ ë‚´ìš©ì„ Vector DBì— ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        try:
            logger.info(f"Starting to add documents from web for query: '{query}'")
            # 1. ë‰´ìŠ¤ ê²€ìƒ‰
            news_results = search_news(query, max_results)
            if not news_results:
                return 0, "No news articles found."

            # 2. ë¬¸ì„œ ìƒì„± (ìŠ¤í¬ë˜í•‘)
            documents = create_documents_from_news(news_results)
            if not documents:
                return 0, "Failed to create documents from news articles."

            # 3. í…ìŠ¤íŠ¸ ë¶„í•  (ì„¤ì •ì—ì„œ ì½ì–´ì˜´)
            external_web_config = config.external_web_rag_config
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=external_web_config['chunk_size'], 
                chunk_overlap=external_web_config['chunk_overlap']
            )
            chunks = text_splitter.split_documents(documents)
            logger.info(f"Created {len(chunks)} chunks from {len(documents)} documents.")

            # 4. DBì— ì¶”ê°€ (persist ì—ëŸ¬ ë°©ì§€)
            if chunks:
                try:
                    self.db.add_documents(chunks)
                    logger.info(f"Successfully added {len(chunks)} new chunks to the vector database.")
                    return len(chunks), f"Successfully added {len(chunks)} new chunks to the database."
                except Exception as persist_error:
                    logger.warning(f"Persist error (ignoring): {persist_error}")
                    # persist ì‹¤íŒ¨í•´ë„ ë©”ëª¨ë¦¬ì—ëŠ” ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                    return len(chunks), f"Successfully added {len(chunks)} new chunks to the database (in-memory)."
            else:
                return 0, "No processable content found in the articles."
        except Exception as e:
            logger.error(f"Error adding documents from web: {e}")
            return 0, f"An error occurred: {str(e)}"

    def _setup_rag_chain(self):
        try:
            # RAG ì²´ì¸ì„ ìˆ˜ë™ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ChatPromptValue ë¬¸ì œ í•´ê²°
            logger.info("Setting up simplified RAG chain...")

            # RAG chainì€ Noneìœ¼ë¡œ ì„¤ì •í•˜ê³ , generate_responseì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬
            self.rag_chain = None

            # ê°œì„ ëœ ë³´ê³ ì„œ í…œí”Œë¦¿ ì •ì˜
            self.report_template = '''ë‹¹ì‹ ì€ í•œêµ­ì˜ ì „ë¬¸ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

ğŸ“‹ ë¶„ì„ ì§€ì¹¨:
- ì œê³µëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ì¢…í•©í•˜ì—¬ ì²´ê³„ì ì¸ ë³´ê³ ì„œ ì‘ì„±
- ì„œë¡œ ë‹¤ë¥¸ ê´€ì ê³¼ ì •ë³´ë¥¼ ê· í˜•ìˆê²Œ ì œì‹œ
- ì¤‘ë³µ ë‚´ìš© ìµœì†Œí™”, ë‹¤ì–‘í•œ ì‹œê° í¬í•¨
- ì „ì²´ ì‘ë‹µì„ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±

ğŸ“° ë‰´ìŠ¤ ê¸°ì‚¬ ìë£Œ:
{context}

ğŸ“‹ ì§ˆë¬¸: {question}

ë‹¤ìŒ êµ¬ì¡°ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”:

## ğŸ“Š í•µì‹¬ ìš”ì•½
(ì§ˆë¬¸ì˜ í•µì‹¬ ë‹µë³€ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ)

## ğŸ” ìƒì„¸ ë¶„ì„
(ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì˜ ì£¼ìš” ë‚´ìš©ì„ ë¶„ì„. ì„œë¡œ ë‹¤ë¥¸ ê´€ì  í¬í•¨)

## ğŸ“ˆ í˜„í™© ë° ë™í–¥
â€¢ í˜„ì¬ ìƒí™©: (ìµœì‹  í˜„í™©)
â€¢ ì£¼ìš” ë³€í™”: (ìµœê·¼ ë³€í™” ì‚¬í•­)
â€¢ í–¥í›„ ì „ë§: (ë¯¸ë˜ ì˜ˆì¸¡)

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸
â€¢ (ê°€ì¥ ì¤‘ìš”í•œ ì‚¬ì‹¤ 3-5ê°œ)

## ğŸ“š ì°¸ê³ ìë£Œ
(ë¶„ì„ì— ì‚¬ìš©ëœ ì£¼ìš” ê¸°ì‚¬ ì œëª©ê³¼ ì¶œì²˜)'''

            logger.info("RAG chain setup completed successfully")

        except Exception as e:
            logger.error(f"Error setting up RAG chain: {e}")
            raise

    def format_docs(self, docs):
        """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ë³´ê³ ì„œ ì‘ì„±ì— ì í•©í•œ í˜•íƒœë¡œ í¬ë§·íŒ… (ìµœì í™”ë¨)"""
        formatted_docs = []
        for i, doc in enumerate(docs, 1):
            # ë‚´ìš© ê¸¸ì´ë¥¼ 500ìë¡œ ì œí•œí•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ìµœì í™”
            content = doc.page_content.strip()
            if len(content) > 500:
                content = content[:500] + "..."

            formatted_doc = f"""[ê¸°ì‚¬{i}] {doc.metadata.get('title', 'Unknown')}
ì¶œì²˜: {doc.metadata.get('source', 'Unknown')[:50]}...
ë‚´ìš©: {content}"""

            formatted_docs.append(formatted_doc)

        return "\n\n".join(formatted_docs)

    def generate_response(self, query: str) -> str:
        try:
            if self.retriever is None:
                logger.error("RAG retriever not initialized")
                return "RAG service not initialized"

            logger.info(f"Generating RAG response for query: {query}")

            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            docs = self.retriever.invoke(query)
            if not docs:
                logger.warning("No relevant documents found")
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê´€ë ¨ ì£¼ì œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."

            logger.info(f"Found {len(docs)} relevant documents")

            # 2. ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
            context = self.format_docs(docs)

            # 3. ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            final_prompt = self.report_template.format(context=context, question=query)

            logger.info("Generating response with LLM...")
            logger.info(f"Prompt length: {len(final_prompt)} characters")

            # 4. LLMìœ¼ë¡œ ì§ì ‘ ìƒì„± (ì„±ëŠ¥ ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)
            response = self.llm_handler.generate(
                final_prompt,
                max_length=1024,  # ê¸¸ì´ ë‹¨ì¶•ìœ¼ë¡œ ì†ë„ í–¥ìƒ
                temperature=0.3,  # ì¼ê´€ì„± ìˆëŠ” í•œêµ­ì–´ ì‘ë‹µ
                stream=False
            )

            logger.info("Successfully generated RAG response")
            logger.info(f"Response length: {len(response)} characters")

            return response

        except Exception as e:
            logger.error(f"Error generating RAG response: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            return f"Error: ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ({str(e)})"

    def auto_search_and_respond(self, query: str, max_results: int = 10) -> dict:
        """
        ì§ˆì˜ì— ëŒ€í•´ ìë™ìœ¼ë¡œ ì›¹ ê²€ìƒ‰í•˜ì—¬ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ê³  ë²¡í„° DBí™” í•œ í›„ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            query: ì‚¬ìš©ìì˜ ì§ˆì˜
            max_results: ê²€ìƒ‰í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜

        Returns:
            dict: ì‘ë‹µ, ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜, ê´€ë ¨ ë¬¸ì„œ ì •ë³´ ë“±ì„ í¬í•¨
        """
        try:
            logger.info(f"ìë™ ì›¹ ê²€ìƒ‰ ê¸°ë°˜ RAG ì‘ë‹µ ìƒì„± ì‹œì‘: '{query}'")

            # 1. ì§ˆì˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›¹ì—ì„œ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë²¡í„° DB ì¶”ê°€
            logger.info(f"'{query}' ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì›¹ì—ì„œ ìë™ ê²€ìƒ‰ ì¤‘...")
            added_chunks, upload_message = self.add_documents_from_web(query, max_results)

            if added_chunks == 0:
                logger.warning(f"'{query}'ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "response": f"'{query}'ì— ëŒ€í•œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "added_chunks": 0,
                    "relevant_documents": [],
                    "search_query": query,
                    "status": "no_results_found"
                }

            logger.info(f"ì›¹ ê²€ìƒ‰ ì™„ë£Œ. {added_chunks}ê°œì˜ ìƒˆë¡œìš´ ì²­í¬ê°€ ë²¡í„° DBì— ì¶”ê°€ë¨")

            # 2. ìƒˆë¡œ ì¶”ê°€ëœ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ RAG ì‘ë‹µ ìƒì„±
            logger.info("ì—…ë°ì´íŠ¸ëœ ë²¡í„° DBë¥¼ ê¸°ë°˜ìœ¼ë¡œ RAG ì‘ë‹µ ìƒì„± ì¤‘...")
            response = self.generate_response(query)

            # 3. ê´€ë ¨ ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            relevant_docs = self.get_relevant_documents(query, k=8)

            logger.info("ìë™ ì›¹ ê²€ìƒ‰ ê¸°ë°˜ RAG ì‘ë‹µ ìƒì„± ì™„ë£Œ")

            return {
                "response": response,
                "added_chunks": added_chunks,
                "relevant_documents": relevant_docs,
                "search_query": query,
                "upload_message": upload_message,
                "status": "success"
            }

        except Exception as e:
            logger.error(f"ìë™ ì›¹ ê²€ìƒ‰ RAG ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            return {
                "response": f"ìë™ ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "added_chunks": 0,
                "relevant_documents": [],
                "search_query": query,
                "status": "error"
            }

    def get_relevant_documents(self, query: str, k: int = 8):
        try:
            if self.retriever is None:
                return []

            # ì‚¬ìš©ì ì§€ì • k ê°’ìœ¼ë¡œ ê²€ìƒ‰
            custom_retriever = self.db.as_retriever(search_kwargs={'k': k})
            docs = custom_retriever.invoke(query)

            # ë¬¸ì„œ ì •ë³´ë¥¼ ë” ìì„¸íˆ ë°˜í™˜
            formatted_docs = []
            for doc in docs:
                formatted_docs.append({
                    "title": doc.metadata.get('title', 'Unknown'),
                    "content": doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content,
                    "source": doc.metadata.get('source', 'Unknown'),
                    "category": doc.metadata.get('category', 'Unknown'),
                    "date": doc.metadata.get('date', 'Unknown'),
                    "score": doc.metadata.get('score', 0)
                })

            return formatted_docs

        except Exception as e:
            logger.error(f"Error retrieving documents: {e}")
            return []

    # === ìƒˆë¡œìš´ ë‰´ìŠ¤ ê´€ë ¨ ê¸°ëŠ¥ë“¤ ===
    
    def get_latest_news(self, categories: list = None, max_results: int = 10, time_range: str = 'd'):
        """
        ìµœì‹  ë‰´ìŠ¤ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ì§€ ì•Šê³  ì¡°íšŒë§Œ)
        """
        try:
            logger.info(f"ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ ì¤‘... ì¹´í…Œê³ ë¦¬: {categories}, ê²°ê³¼ìˆ˜: {max_results}")
            news_results = search_latest_news(max_results=max_results, categories=categories, time_range=time_range)
            
            formatted_news = []
            for news in news_results:
                formatted_news.append({
                    'title': news.get('title', ''),
                    'url': news.get('url', ''),
                    'content': news.get('content', ''),
                    'category': news.get('category', 'general'),
                    'published_date': news.get('published_date', ''),
                    'score': news.get('score', 0)
                })
            
            return formatted_news
            
        except Exception as e:
            logger.error(f"ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def summarize_news(self, query: str, max_results: int = 5, summary_type: str = "comprehensive"):
        """
        íŠ¹ì • ì£¼ì œì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  LLMì„ ì´ìš©í•´ ìš”ì•½í•©ë‹ˆë‹¤
        
        Args:
            query: ê²€ìƒ‰í•  ì£¼ì œ
            max_results: ë¶„ì„í•  ë‰´ìŠ¤ ê°œìˆ˜
            summary_type: ìš”ì•½ íƒ€ì… ("brief", "comprehensive", "analysis")
        """
        try:
            logger.info(f"'{query}' ì£¼ì œ ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì¤‘...")
            
            # Tavilyë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ê¸°ë³¸ ìš”ì•½ íšë“
            news_data = get_news_summary_with_tavily(query, max_results)
            
            if not news_data:
                return {
                    "summary": f"'{query}' ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "articles": [],
                    "keywords": [],
                    "sentiment": "neutral"
                }
            
            # ìš”ì•½ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            summary_prompts = {
                "brief": self._get_brief_summary_prompt(),
                "comprehensive": self._get_comprehensive_summary_prompt(),
                "analysis": self._get_analysis_summary_prompt()
            }
            
            prompt_template = summary_prompts.get(summary_type, summary_prompts["comprehensive"])
            
            # ë‰´ìŠ¤ ë°ì´í„° ì¤€ë¹„
            articles_text = "\n\n".join([
                f"ì œëª©: {article.get('title', '')}\në‚´ìš©: {article.get('content', '')[:1000]}"
                for article in news_data[:max_results]
                if not article.get('is_summary', False)  # Tavilyì˜ ìë™ ìš”ì•½ ì œì™¸
            ])
            
            # LLMìœ¼ë¡œ ìš”ì•½ ìƒì„±
            full_prompt = prompt_template.format(query=query, articles=articles_text)
            summary_response = self.llm_handler.chat_model.invoke(full_prompt)
            
            return {
                "summary": summary_response.content if hasattr(summary_response, 'content') else str(summary_response),
                "articles": news_data[:max_results],
                "query": query,
                "summary_type": summary_type,
                "total_articles": len(news_data)
            }
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                "summary": f"ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "articles": [],
                "keywords": [],
                "sentiment": "neutral"
            }
    
    def analyze_news_trends(self, categories: list = None, max_results: int = 20, time_range: str = 'd'):
        """
        ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ íŠ¸ë Œë“œë¥¼ íŒŒì•…í•©ë‹ˆë‹¤
        """
        try:
            logger.info(f"ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘... ì¹´í…Œê³ ë¦¬: {categories}")
            
            if categories is None:
                categories = ['politics', 'economy', 'technology', 'society']
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
            all_news = []
            category_summaries = {}
            
            for category in categories:
                category_news = search_news(
                    "ìµœì‹  ë‰´ìŠ¤", 
                    max_results=max_results//len(categories), 
                    category=category,
                    time_range=time_range
                )
                
                if category_news:
                    all_news.extend(category_news)
                    
                    # ì¹´í…Œê³ ë¦¬ë³„ ê°„ë‹¨ ìš”ì•½
                    category_text = "\n".join([
                        f"â€¢ {news.get('title', '')}: {news.get('content', '')[:200]}"
                        for news in category_news[:3]
                    ])
                    
                    category_prompt = f"ë‹¤ìŒ {category} ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ë“¤ì˜ ì£¼ìš” íŠ¸ë Œë“œë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n{category_text}"
                    category_summary = self.llm_handler.chat_model.invoke(category_prompt)
                    category_summaries[category] = category_summary.content if hasattr(category_summary, 'content') else str(category_summary)
            
            # ì „ì²´ íŠ¸ë Œë“œ ë¶„ì„
            trend_analysis_prompt = self._get_trend_analysis_prompt()
            all_titles = [news.get('title', '') for news in all_news]
            titles_text = "\n".join([f"â€¢ {title}" for title in all_titles[:30]])
            
            full_trend_prompt = trend_analysis_prompt.format(
                titles=titles_text,
                category_summaries="\n".join([f"{cat}: {summary}" for cat, summary in category_summaries.items()])
            )
            
            trend_response = self.llm_handler.chat_model.invoke(full_trend_prompt)
            
            return {
                "overall_trend": trend_response.content if hasattr(trend_response, 'content') else str(trend_response),
                "category_trends": category_summaries,
                "total_articles_analyzed": len(all_news),
                "categories": categories,
                "time_range": time_range
            }
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "overall_trend": f"íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "category_trends": {},
                "total_articles_analyzed": 0
            }

    # === í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ë“¤ ===
    
    def _get_brief_summary_prompt(self):
        """ê°„ë‹¨ ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸"""
        return """ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{query}' ì£¼ì œì— ëŒ€í•œ ê°„ë‹¨í•œ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ê¸°ì‚¬ë“¤:
{articles}

ìš”êµ¬ì‚¬í•­:
1. í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ìš”ì•½
2. ê°€ì¥ ì¤‘ìš”í•œ í¬ì¸íŠ¸ë§Œ í¬í•¨
3. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±
4. í•œêµ­ì–´ë¡œ ì‘ì„±

ê°„ë‹¨ ìš”ì•½:"""

    def _get_comprehensive_summary_prompt(self):
        """í¬ê´„ì  ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸"""
        return """ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{query}' ì£¼ì œì— ëŒ€í•œ í¬ê´„ì ì¸ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ê¸°ì‚¬ë“¤:
{articles}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸ“° ì£¼ìš” ë‚´ìš© ìš”ì•½
(í•µì‹¬ ë‚´ìš©ì„ 3-4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½)

## ğŸ” ì„¸ë¶€ ë¶„ì„
â€¢ ì£¼ìš” ì´ìŠˆ: 
â€¢ ê´€ë ¨ ì¸ë¬¼/ê¸°ê´€:
â€¢ ì˜í–¥/ê²°ê³¼:

## ğŸ·ï¸ í‚¤ì›Œë“œ
(ê´€ë ¨ í‚¤ì›Œë“œ 3-5ê°œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„)

## ğŸ“Š ì¢…í•© í‰ê°€
(ì „ë°˜ì ì¸ ìƒí™© í‰ê°€ì™€ í–¥í›„ ì „ë§ 1-2ë¬¸ì¥)

ëª¨ë“  ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    def _get_analysis_summary_prompt(self):
        """ë¶„ì„ ì¤‘ì‹¬ ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸"""
        return """ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{query}' ì£¼ì œì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ê¸°ì‚¬ë“¤:
{articles}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

## ğŸ¯ í•µì‹¬ ì´ìŠˆ ë¶„ì„
(ê°€ì¥ ì¤‘ìš”í•œ ì´ìŠˆì™€ ê·¸ ë°°ê²½)

## ğŸ“ˆ í˜„í™© ë° íŠ¸ë Œë“œ
â€¢ í˜„ì¬ ìƒí™©:
â€¢ ë³€í™” ì¶”ì´:
â€¢ ì£¼ëª©í•  ì :

## âš¡ ì£¼ìš” ë™í–¥
â€¢ ê¸ì •ì  ìš”ì†Œ:
â€¢ ìš°ë ¤ì‚¬í•­:
â€¢ ì˜ˆìƒ ì‹œë‚˜ë¦¬ì˜¤:

## ğŸŒŸ ì‹œì‚¬ì  ë° ì „ë§
(ì´ ë‰´ìŠ¤ê°€ ê°–ëŠ” ì˜ë¯¸ì™€ í–¥í›„ ì˜ˆìƒë˜ëŠ” ë°œì „ ë°©í–¥)

## ğŸ·ï¸ í•µì‹¬ í‚¤ì›Œë“œ
(ë¶„ì„ì— ì¤‘ìš”í•œ í‚¤ì›Œë“œ 5-7ê°œ)

ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ ì‹œê°ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    def _get_trend_analysis_prompt(self):
        """íŠ¸ë Œë“œ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸"""
        return """ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ë“¤ê³¼ ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ì œëª©ë“¤:
{titles}

ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½:
{category_summaries}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

## ğŸ”¥ ì˜¤ëŠ˜ì˜ ì£¼ìš” íŠ¸ë Œë“œ
(ê°€ì¥ ì£¼ëª©ë°›ëŠ” ì´ìŠˆ 2-3ê°œ)

## ğŸ“Š ë¶„ì•¼ë³„ ë™í–¥
â€¢ ì •ì¹˜: (ì •ì¹˜ ê´€ë ¨ ì£¼ìš” ì´ìŠˆ)
â€¢ ê²½ì œ: (ê²½ì œ ê´€ë ¨ ì£¼ìš” ì´ìŠˆ)  
â€¢ ì‚¬íšŒ: (ì‚¬íšŒ ê´€ë ¨ ì£¼ìš” ì´ìŠˆ)
â€¢ ê¸°ìˆ : (ê¸°ìˆ  ê´€ë ¨ ì£¼ìš” ì´ìŠˆ)

## ğŸ­ ì—¬ë¡  ë° ê´€ì‹¬ë„
(êµ­ë¯¼ë“¤ì´ ê°€ì¥ ê´€ì‹¬ ê°–ëŠ” ì´ìŠˆë“¤ê³¼ ì—¬ë¡ ì˜ ë°©í–¥)

## ğŸ”® ì£¼ëª©í•  í¬ì¸íŠ¸
(ì•ìœ¼ë¡œ ê³„ì† ì£¼ëª©í•´ì•¼ í•  ì´ìŠˆë“¤)

ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ì‹œê°ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
