import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.config import settings
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from models.llm_handler import LLMHandler
from models.embedding_handler import EmbeddingHandler
from services.rag_service import RAGService
from services.internal_db_service import InternalDBService
import logging
import torch
import json
import asyncio

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter()

# Request models
class GenerateRequest(BaseModel):
    prompt: str
    max_length: int = 512
    model_key: str = None  # ì‚¬ìš©í•  ëª¨ë¸ í‚¤
    stream: bool = False  # ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€

class EmbeddingRequest(BaseModel):
    text: str

class RAGRequest(BaseModel):
    question: str
    model_key: str = None  # ì‚¬ìš©í•  ëª¨ë¸ í‚¤

class RAGUpdateRequest(BaseModel):
    query: str
    max_results: int = 5


class ChatRequest(BaseModel):
    message: str
    model_key: str = None  # ì‚¬ìš©í•  ëª¨ë¸ í‚¤
    stream: bool = False  # ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€

class ModelSwitchRequest(BaseModel):
    model_key: str  # ì „í™˜í•  ëª¨ë¸ í‚¤

# Request models for new endpoints
class ModelRecommendationRequest(BaseModel):
    ram_gb: int = None
    gpu_gb: int = None
    use_case: str = None  # korean, coding, math, multilingual

class PerformanceComparisonRequest(BaseModel):
    model_keys: list[str] = None

# ìƒˆë¡œìš´ ë‰´ìŠ¤ ê´€ë ¨ ìš”ì²­ ëª¨ë¸ë“¤
class NewsSummaryRequest(BaseModel):
    query: str
    max_results: int = 5
    summary_type: str = "comprehensive"  # brief, comprehensive, analysis
    model_key: str = None

class NewsAnalysisRequest(BaseModel):
    categories: list[str] = None
    max_results: int = 20
    time_range: str = 'd'
    model_key: str = None

class LatestNewsRequest(BaseModel):
    categories: list[str] = None
    max_results: int = 10
    time_range: str = 'd'

# External-Web RAG ìš”ì²­ ëª¨ë¸ë“¤
class ExternalWebUploadRequest(BaseModel):
    topic: str
    max_results: int = 20

class ExternalWebQueryRequest(BaseModel):
    prompt: str
    top_k: int = 5
    model_key: str = None

# Internal-DBMS RAG ìš”ì²­ ëª¨ë¸ë“¤
class InternalDBIngestRequest(BaseModel):
    table: str = "knowledge"
    save_name: str = "knowledge"
    simulate: bool = False
    id_col: str = None
    title_col: str = None
    text_cols: list[str] = None

class InternalDBQueryRequest(BaseModel):
    save_name: str = "knowledge"
    question: str
    top_k: int = 5
    margin: float = 0.12

# Initialize handlers (lazy loading)
llm_handler = None
embedding_handler = None
rag_service = None
internal_db_service = None

def get_llm_handler(model_key: str = None):
    global llm_handler
    
    # ëª¨ë¸ í‚¤ê°€ ì§€ì •ë˜ì—ˆê³  í˜„ì¬ í•¸ë“¤ëŸ¬ì™€ ë‹¤ë¥¸ ê²½ìš° ìƒˆë¡œ ìƒì„±
    if model_key and (llm_handler is None or getattr(llm_handler, 'model_key', None) != model_key):
        logger.info(f"ëª¨ë¸ ì „í™˜ ì¤‘: {model_key}")
        llm_handler = LLMHandler(model_key=model_key)
    elif llm_handler is None:
        logger.info("ê¸°ë³¸ LLM í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” ì¤‘...")
        llm_handler = LLMHandler()
    
    return llm_handler

def get_embedding_handler():
    global embedding_handler
    if embedding_handler is None:
        logger.info("Initializing embedding handler...")
        embedding_handler = EmbeddingHandler()
    return embedding_handler

def get_rag_service(model_key: str = None):
    global rag_service
    if rag_service is None or model_key:
        logger.info("RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        llm = get_llm_handler(model_key)
        emb = get_embedding_handler()
        rag_service = RAGService(llm, emb)
    return rag_service

def get_internal_db_service():
    global internal_db_service
    if internal_db_service is None:
        logger.info("Internal DB ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        llm = get_llm_handler()
        emb = get_embedding_handler()
        internal_db_service = InternalDBService(llm, emb)
    return internal_db_service

@router.post("/generate")
async def generate_response(request: GenerateRequest):
    try:
        handler = get_llm_handler(request.model_key)
        
        if request.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (SSE í˜•ì‹)
            async def generate_stream():
                import asyncio
                for chunk in handler.generate(request.prompt, request.max_length, stream=True):
                    if chunk:
                        # SSE í˜•ì‹ìœ¼ë¡œ ë°ì´í„° í¬ë§·íŒ…
                        data = json.dumps({"content": chunk, "done": False}, ensure_ascii=False)
                        yield f"data: {data}\n\n"
                        await asyncio.sleep(0)  # ì¦‰ì‹œ yieldí•˜ë„ë¡ í•¨
                # ì™„ë£Œ ì‹ í˜¸
                final_data = json.dumps({"content": "", "done": True})
                yield f"data: {final_data}\n\n"
            
            return StreamingResponse(
                generate_stream(), 
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no"  # nginx ë²„í¼ë§ ë°©ì§€
                }
            )
        else:
            # ì¼ë°˜ ì‘ë‹µ
            response = handler.generate(request.prompt, request.max_length, stream=False)
            return {
                "response": response, 
                "prompt": request.prompt,
                "model_info": {
                    "model_key": handler.model_key,
                    "model_id": handler.SUPPORTED_MODELS[handler.model_key]["model_id"],
                    "description": handler.SUPPORTED_MODELS[handler.model_key]["description"],
                    "category": handler.SUPPORTED_MODELS[handler.model_key]["category"],
                    "loaded": handler.model is not None
                }
            }
    except Exception as e:
        logger.error(f"ìƒì„± ì—”ë“œí¬ì¸íŠ¸ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/chat")
async def chat_response(request: ChatRequest):
    try:
        handler = get_llm_handler(request.model_key)
        
        if request.stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (SSE í˜•ì‹)
            async def chat_stream():
                for chunk in handler.chat_generate(request.message, stream=True):
                    if chunk:
                        # SSE í˜•ì‹ìœ¼ë¡œ ë°ì´í„° í¬ë§·íŒ…
                        data = json.dumps({"content": chunk, "done": False})
                        yield f"data: {data}\n\n"
                # ì™„ë£Œ ì‹ í˜¸
                final_data = json.dumps({"content": "", "done": True})
                yield f"data: {final_data}\n\n"
            
            return StreamingResponse(
                chat_stream(), 
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive"
                }
            )
        else:
            # ì¼ë°˜ ì‘ë‹µ
            response = handler.chat_generate(request.message, stream=False)
            return {
                "response": response, 
                "message": request.message,
                "model_info": {
                    "model_key": handler.model_key,
                    "model_id": handler.SUPPORTED_MODELS[handler.model_key]["model_id"],
                    "description": handler.SUPPORTED_MODELS[handler.model_key]["description"],
                    "category": handler.SUPPORTED_MODELS[handler.model_key]["category"],
                    "loaded": handler.model is not None
                }
            }
    except Exception as e:
        logger.error(f"ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/embed")
async def create_embedding(request: EmbeddingRequest):
    try:
        handler = get_embedding_handler()
        embedding = handler.create_embedding(request.text)
        return {"embedding": embedding, "text": request.text, "dimension": len(embedding)}
    except Exception as e:
        logger.error(f"Error in embed endpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/rag")
async def rag_response(request: RAGRequest):
    try:
        service = get_rag_service(request.model_key)
        response = service.generate_response(request.question)
        relevant_docs = service.get_relevant_documents(request.question)
        return {
            "response": response, 
            "question": request.question,
            "relevant_documents": relevant_docs,
            "model_info": {
                "model_key": service.llm_handler.model_key,
                "model_id": service.llm_handler.SUPPORTED_MODELS[service.llm_handler.model_key]["model_id"],
                "description": service.llm_handler.SUPPORTED_MODELS[service.llm_handler.model_key]["description"],
                "category": service.llm_handler.SUPPORTED_MODELS[service.llm_handler.model_key]["category"],
                "loaded": service.llm_handler.model is not None
            }
        }
    except Exception as e:
        logger.error(f"RAG ì—”ë“œí¬ì¸íŠ¸ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/rag/update-news")
async def update_rag_with_news(request: RAGUpdateRequest):
    """
    ì›¹ì—ì„œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ RAG ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    try:
        service = get_rag_service() # ê¸°ë³¸ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        added_chunks, message = service.add_documents_from_web(request.query, request.max_results)
        
        if added_chunks > 0:
            return {"message": message, "added_chunks": added_chunks}
        else:
            # ë¬¸ì„œ ì¶”ê°€ì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ì°¾ì§€ ëª»í•œ ê²½ìš°
            raise HTTPException(status_code=404, detail=message)
            
    except HTTPException as e:
        raise e # HTTP ì˜ˆì™¸ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ ì—”ë“œí¬ì¸íŠ¸ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {str(e)}")


@router.get("/health")
async def health_check():
    current_model_info = None
    if llm_handler:
        current_model_info = {
            "model_key": llm_handler.model_key,
            "model_id": llm_handler.SUPPORTED_MODELS[llm_handler.model_key]["model_id"],
            "description": llm_handler.SUPPORTED_MODELS[llm_handler.model_key]["description"],
            "category": llm_handler.SUPPORTED_MODELS[llm_handler.model_key]["category"],
            "loaded": llm_handler.model is not None
        }
    
    return {
        "status": "healthy",
        "llm_loaded": llm_handler is not None,
        "embedding_loaded": embedding_handler is not None,
        "rag_loaded": rag_service is not None,
        "current_model": current_model_info,
        "gpu_available": torch.cuda.is_available(),
        "gpu_memory": torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else None
    }

@router.get("/models")
async def list_models():
    """ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    # config.pyì—ì„œ ì •ì˜ëœ ëª¨ë¸ ëª©ë¡ì„ ì‚¬ìš©
    model_keys = settings.available_models
    
    # Gradioê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    # ê° ëª¨ë¸ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€ ë¡œì§ì´ í•„ìš”í•˜ì§€ë§Œ,
    # í˜„ì¬ëŠ” í‚¤ ëª©ë¡ë§Œ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
    models_dict = {key: {"model_id": key} for key in model_keys}
    
    return {
        "supported_models": models_dict,
        "total_models": len(model_keys)
    }

@router.get("/models/categories")
async def get_model_categories():
    """ëª¨ë¸ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
    try:
        categories = LLMHandler.get_model_categories()
        models_by_category = LLMHandler.get_models_by_category()
        
        return {
            "categories": categories,
            "models_by_category": models_by_category,
            "category_descriptions": {
                "ultra-light": "0.5GB ë¯¸ë§Œì˜ ì´ˆê²½ëŸ‰ ëª¨ë¸ë“¤",
                "light": "1-6GB RAM, ë¹ ë¥¸ ì‘ë‹µê³¼ íš¨ìœ¨ì„± ì¤‘ì‹¬",
                "medium": "7-20GB RAM, ì„±ëŠ¥ê³¼ íš¨ìœ¨ì˜ ê· í˜•",
                "large": "30GB+ RAM, ìµœê³  ì„±ëŠ¥ì˜ ëŒ€í˜• ëª¨ë¸",
                "korean": "í•œêµ­ì–´ì— íŠ¹í™”ëœ ëª¨ë¸ë“¤",
                "code": "í”„ë¡œê·¸ë˜ë°ê³¼ ì½”ë”©ì— íŠ¹í™”ëœ ëª¨ë¸ë“¤",
                "math": "ìˆ˜í•™ê³¼ ê³¼í•™ ê³„ì‚°ì— íŠ¹í™”ëœ ëª¨ë¸ë“¤",
                "multilingual": "ë‹¤êµ­ì–´ ì§€ì›ì— ê°•í•œ ëª¨ë¸ë“¤"
            }
        }
    except Exception as e:
        logger.error(f"ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/models/category/{category}")
async def get_models_by_category(category: str):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë¸ë“¤ ë°˜í™˜"""
    try:
        models = LLMHandler.get_models_by_category(category)
        if not models:
            raise HTTPException(status_code=404, detail=f"ì¹´í…Œê³ ë¦¬ '{category}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
        return {
            "category": category,
            "models": models,
            "count": len(models)
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/models/recommend")
async def recommend_models(request: ModelRecommendationRequest):
    """ì‹œìŠ¤í…œ ì‚¬ì–‘ê³¼ ìš©ë„ì— ë§ëŠ” ëª¨ë¸ ì¶”ì²œ"""
    try:
        recommendations = LLMHandler.recommend_model(
            ram_gb=request.ram_gb,
            gpu_gb=request.gpu_gb,
            use_case=request.use_case
        )
        
        return {
            "recommendations": recommendations,
            "criteria": {
                "ram_gb": request.ram_gb,
                "gpu_gb": request.gpu_gb,
                "use_case": request.use_case
            },
            "total_recommendations": len(recommendations)
        }
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì¶”ì²œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/models/compare")
async def compare_models(request: PerformanceComparisonRequest):
    """ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ ë¹„êµ"""
    try:
        comparison = LLMHandler.get_performance_comparison(request.model_keys)
        
        return {
            "comparison": comparison,
            "total_models": len(comparison),
            "sorted_by": "performance_score"
        }
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¹„êµ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/models/search")
async def search_models(
    category: str = None,
    min_ram: int = None,
    max_ram: int = None,
    min_gpu: int = None,
    max_gpu: int = None,
    keyword: str = None
):
    """ëª¨ë¸ ê²€ìƒ‰ ë° í•„í„°ë§"""
    try:
        all_models = LLMHandler.get_supported_models()
        filtered_models = {}
        
        for key, model in all_models.items():
            # ì¹´í…Œê³ ë¦¬ í•„í„°
            if category and model.get("category") != category:
                continue
                
            # RAM ìš”êµ¬ì‚¬í•­ í•„í„°
            model_ram = int(model["ram_requirement"].replace("GB", ""))
            if min_ram and model_ram < min_ram:
                continue
            if max_ram and model_ram > max_ram:
                continue
                
            # GPU ìš”êµ¬ì‚¬í•­ í•„í„°
            model_gpu = int(model["gpu_requirement"].replace("GB", ""))
            if min_gpu and model_gpu < min_gpu:
                continue
            if max_gpu and model_gpu > max_gpu:
                continue
                
            # í‚¤ì›Œë“œ ê²€ìƒ‰
            if keyword:
                search_text = f"{key} {model['description']} {model['model_id']}".lower()
                if keyword.lower() not in search_text:
                    continue
                    
            filtered_models[key] = model
        
        return {
            "models": filtered_models,
            "total_found": len(filtered_models),
            "filters": {
                "category": category,
                "min_ram": min_ram,
                "max_ram": max_ram,
                "min_gpu": min_gpu,
                "max_gpu": max_gpu,
                "keyword": keyword
            }
        }
    except Exception as e:
        logger.error(f"ëª¨ë¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/models/stats")
async def get_model_statistics():
    """ëª¨ë¸ í†µê³„ ì •ë³´"""
    try:
        all_models = LLMHandler.get_supported_models()
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        ram_stats = {"min": float('inf'), "max": 0, "avg": 0}
        gpu_stats = {"min": float('inf'), "max": 0, "avg": 0}
        
        total_ram = 0
        total_gpu = 0
        
        for model in all_models.values():
            # ì¹´í…Œê³ ë¦¬ í†µê³„
            category = model.get("category", "other")
            category_stats[category] = category_stats.get(category, 0) + 1
            
            # RAM/GPU í†µê³„
            ram_req = int(model["ram_requirement"].replace("GB", ""))
            gpu_req = int(model["gpu_requirement"].replace("GB", ""))
            
            ram_stats["min"] = min(ram_stats["min"], ram_req)
            ram_stats["max"] = max(ram_stats["max"], ram_req)
            total_ram += ram_req
            
            gpu_stats["min"] = min(gpu_stats["min"], gpu_req)
            gpu_stats["max"] = max(gpu_stats["max"], gpu_req)
            total_gpu += gpu_req
        
        model_count = len(all_models)
        ram_stats["avg"] = round(total_ram / model_count, 2)
        gpu_stats["avg"] = round(total_gpu / model_count, 2)
        
        return {
            "total_models": model_count,
            "category_distribution": category_stats,
            "ram_requirements": ram_stats,
            "gpu_requirements": gpu_stats,
            "model_size_distribution": {
                "small (â‰¤3B)": len([k for k in all_models.keys() if any(size in k for size in ["0.5b", "1b", "1.5b", "2b", "3b"])]),
                "medium (4-20B)": len([k for k in all_models.keys() if any(size in k for size in ["7b", "8b", "9b", "10b", "12b", "13b", "14b"])]),
                "large (>20B)": len([k for k in all_models.keys() if any(size in k for size in ["32b", "33b", "70b", "72b"])])
            }
        }
    except Exception as e:
        logger.error(f"ëª¨ë¸ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/models/info/{model_key}")
async def get_model_info_endpoint(model_key: str):
    """íŠ¹ì • ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
    model_info = LLMHandler.get_model_info(model_key)
    if not model_info:
        raise HTTPException(status_code=404, detail=f"ëª¨ë¸ '{model_key}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return {
        "model_key": model_key,
        "model_info": model_info
    }

@router.post("/models/switch")
async def switch_model(request: ModelSwitchRequest):
    """ëª¨ë¸ ì „í™˜"""
    global llm_handler, rag_service
    
    # ëª¨ë¸ í‚¤ ìœ íš¨ì„± ê²€ì‚¬
    if request.model_key not in LLMHandler.get_supported_models():
        raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸: {request.model_key}")
    
    try:
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì •ë¦¬
        old_model = None
        if llm_handler:
            old_model = {
                "model_key": llm_handler.model_key,
                "model_id": llm_handler.SUPPORTED_MODELS[llm_handler.model_key]["model_id"],
                "description": llm_handler.SUPPORTED_MODELS[llm_handler.model_key]["description"],
                "category": llm_handler.SUPPORTED_MODELS[llm_handler.model_key]["category"],
                "loaded": llm_handler.model is not None
            }
        
        # ìƒˆ ëª¨ë¸ë¡œ ì „í™˜
        logger.info(f"ëª¨ë¸ ì „í™˜ ì¤‘: {request.model_key}")
        llm_handler = LLMHandler(model_key=request.model_key)
        rag_service = None  # RAG ì„œë¹„ìŠ¤ ì¬ì´ˆê¸°í™” í•„ìš”
        
        new_model = {
            "model_key": llm_handler.model_key,
            "model_id": llm_handler.SUPPORTED_MODELS[llm_handler.model_key]["model_id"],
            "description": llm_handler.SUPPORTED_MODELS[llm_handler.model_key]["description"],
            "category": llm_handler.SUPPORTED_MODELS[llm_handler.model_key]["category"],
            "loaded": llm_handler.model is not None
        }
        
        return {
            "message": "ëª¨ë¸ ì „í™˜ ì™„ë£Œ",
            "old_model": old_model,
            "new_model": new_model
        }
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì „í™˜ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ì „í™˜ ì‹¤íŒ¨: {str(e)}")

@router.get("/system/gpu")
async def get_gpu_info():
    """GPU ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
    if not torch.cuda.is_available():
        return {"gpu_available": False, "message": "CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
    
    gpu_info = []
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
        memory_cached = torch.cuda.memory_reserved(i) / 1024**3
        memory_total = props.total_memory / 1024**3
        
        gpu_info.append({
            "device_id": i,
            "name": props.name,
            "total_memory_gb": round(memory_total, 2),
            "allocated_memory_gb": round(memory_allocated, 2),
            "cached_memory_gb": round(memory_cached, 2),
            "free_memory_gb": round(memory_total - memory_cached, 2),
            "compute_capability": f"{props.major}.{props.minor}"
        })
    
    return {
        "gpu_available": True,
        "gpu_count": torch.cuda.device_count(),
        "current_device": torch.cuda.current_device(),
        "gpu_info": gpu_info
    }

# === ìƒˆë¡œìš´ ë‰´ìŠ¤ ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸ë“¤ ===

@router.get("/news/latest")
async def get_latest_news(
    categories: str = None,  # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì¹´í…Œê³ ë¦¬ ë¬¸ìì—´
    max_results: int = 10,
    time_range: str = 'd'
):
    """ìµœì‹  ë‰´ìŠ¤ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì—†ì´)"""
    try:
        from utils.helpers import search_latest_news
        
        # ì¹´í…Œê³ ë¦¬ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        category_list = None
        if categories:
            category_list = [cat.strip() for cat in categories.split(',')]
        
        logger.info(f"ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ - ì¹´í…Œê³ ë¦¬: {category_list}, ê²°ê³¼ìˆ˜: {max_results}")
        
        news_results = search_latest_news(
            max_results=max_results,
            categories=category_list,
            time_range=time_range
        )
        
        return {
            "news": news_results,
            "total_count": len(news_results),
            "categories": category_list or ["politics", "economy", "technology", "society"],
            "time_range": time_range,
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.post("/news/summary")
async def summarize_news(request: NewsSummaryRequest):
    """íŠ¹ì • ì£¼ì œì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  LLMìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤ (ìŠ¤íŠ¸ë¦¬ë°)"""
    
    async def generate_summary_stream():
        try:
            # ì‹œì‘ ì‹ í˜¸
            yield f"data: {json.dumps({'status': 'starting', 'message': 'ë‰´ìŠ¤ ìš”ì•½ì„ ì‹œì‘í•©ë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            
            # RAG ì„œë¹„ìŠ¤ ëŒ€ì‹  ì§ì ‘ LLM í•¸ë“¤ëŸ¬ ì‚¬ìš©
            llm = get_llm_handler(request.model_key)
            
            logger.info(f"ë‰´ìŠ¤ ìš”ì•½ ìš”ì²­ - ì£¼ì œ: {request.query}, íƒ€ì…: {request.summary_type}")
            logger.debug(f"LLM í•¸ë“¤ëŸ¬ ë¡œë“œ ì™„ë£Œ: {llm.model_key}")
            
            yield f"data: {json.dumps({'status': 'searching', 'message': 'Tavilyë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘...'}, ensure_ascii=False)}\n\n"
            
            # ì§ì ‘ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ìš”ì•½
            from utils.helpers import get_news_summary_with_tavily
            
            logger.debug("Tavilyë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘...")
            news_data = get_news_summary_with_tavily(request.query, request.max_results)
            logger.debug(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ. ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(news_data) if news_data else 0}")
            
            if not news_data:
                no_news_message = f"'{request.query}' ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                final_data = {
                    'status': 'completed', 
                    'summary': no_news_message, 
                    'articles': [], 
                    'source_articles': [], 
                    'query': request.query, 
                    'summary_type': request.summary_type, 
                    'total_articles': 0
                }
                yield f"data: {json.dumps(final_data, ensure_ascii=False)}\n\n"
                return
            
            news_count = len(news_data)
            yield f"data: {json.dumps({'status': 'processing', 'message': f'{news_count}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘...'}, ensure_ascii=False)}\n\n"
            
            # ìš”ì•½ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            summary_prompts = {
                "brief": get_brief_summary_prompt(),
                "comprehensive": get_comprehensive_summary_prompt(),
                "analysis": get_analysis_summary_prompt()
            }
            
            prompt_template = summary_prompts.get(request.summary_type, summary_prompts["comprehensive"])
            
            # ë‰´ìŠ¤ ë°ì´í„° ì¤€ë¹„
            articles_text = "\n\n".join([
                f"ì œëª©: {article.get('title', '')}\nì¶œì²˜: {article.get('url', 'Unknown')}\në‚´ìš©: {article.get('content', '')[:1000]}"
                for article in news_data[:request.max_results]
                if not article.get('is_summary', False)  # Tavilyì˜ ìë™ ìš”ì•½ ì œì™¸
            ])
            
            # ì°¸ê³ í•œ ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ ì¤€ë¹„ (ì‘ë‹µìš©)
            source_articles = []
            for article in news_data[:request.max_results]:
                if not article.get('is_summary', False):
                    source_articles.append({
                        'title': article.get('title', ''),
                        'url': article.get('url', ''),
                        'published_date': article.get('published_date', ''),
                        'score': article.get('score', 0)
                    })
            
            yield f"data: {json.dumps({'status': 'generating', 'message': 'LLMì´ ë‰´ìŠ¤ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ì¤‘...'}, ensure_ascii=False)}\n\n"
            
            # LLMìœ¼ë¡œ ìš”ì•½ ìƒì„±
            logger.debug(f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¤€ë¹„ ì™„ë£Œ. ê¸°ì‚¬ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(articles_text)} ë¬¸ì")
            full_prompt = prompt_template.format(query=request.query, articles=articles_text)
            logger.debug(f"ì „ì²´ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(full_prompt)} ë¬¸ì")
            
            logger.info("LLMìœ¼ë¡œ ë‰´ìŠ¤ ìš”ì•½ ìƒì„± ì¤‘...")
            
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ìƒì„±
            summary_stream = llm.generate(full_prompt, max_length=1024, stream=True)
            
            summary_parts = []
            for chunk in summary_stream:
                if chunk:
                    summary_parts.append(chunk)
                    yield f"data: {json.dumps({'status': 'streaming', 'chunk': chunk}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.01)  # ì•½ê°„ì˜ ì§€ì—°ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
            
            summary = ''.join(summary_parts)
            logger.debug("LLM ìš”ì•½ ìƒì„± ì™„ë£Œ")
            
            # ìš”ì•½ì— ì¶œì²˜ ì •ë³´ ì¶”ê°€
            summary_with_sources = summary + "\n\n" + "ğŸ“° **ì°¸ê³  ê¸°ì‚¬:**\n" + "\n".join([
                f"â€¢ [{article['title']}]({article['url']})" + (f" ({article['published_date']})" if article['published_date'] else "")
                for article in source_articles[:5]  # ìµœëŒ€ 5ê°œ ì¶œì²˜ë§Œ í‘œì‹œ
            ])
            
            # ì™„ë£Œ ì‹ í˜¸
            final_result = {
                "status": "completed",
                "summary": summary_with_sources,
                "articles": news_data[:request.max_results],
                "source_articles": source_articles,
                "query": request.query,
                "summary_type": request.summary_type,
                "total_articles": len(news_data),
                "model_info": {
                    "model_key": llm.model_key,
                    "model_id": llm.SUPPORTED_MODELS[llm.model_key]["model_id"]
                }
            }
            
            yield f"data: {json.dumps(final_result, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìš”ì•½ ì˜¤ë¥˜: {e}")
            error_msg = f'ë‰´ìŠ¤ ìš”ì•½ ì‹¤íŒ¨: {str(e)}'
            yield f"data: {json.dumps({'status': 'error', 'message': error_msg}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(generate_summary_stream(), media_type="text/plain; charset=utf-8")

def get_brief_summary_prompt():
    """ê°„ë‹¨ ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸"""
    return """ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{query}' ì£¼ì œì— ëŒ€í•œ ê°„ë‹¨í•œ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ê¸°ì‚¬ë“¤:
{articles}

ìš”êµ¬ì‚¬í•­:
1. í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ìš”ì•½
2. ê°€ì¥ ì¤‘ìš”í•œ í¬ì¸íŠ¸ë§Œ í¬í•¨
3. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±
4. í•œêµ­ì–´ë¡œ ì‘ì„±
5. ì¶œì²˜ ì •ë³´ëŠ” ë³„ë„ë¡œ ì œê³µë˜ë¯€ë¡œ ìš”ì•½ ë³¸ë¬¸ì—ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”

ê°„ë‹¨ ìš”ì•½:"""

def get_comprehensive_summary_prompt():
    """í¬ê´„ì  ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸"""
    return """ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{query}' ì£¼ì œì— ëŒ€í•œ í¬ê´„ì ì¸ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ê¸°ì‚¬ë“¤:
{articles}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸ“° ì£¼ìš” ë‚´ìš© ìš”ì•½
(í•µì‹¬ ë‚´ìš©ì„ 3-4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½)

## ğŸ” ì„¸ë¶€ ë¶„ì„
â€¢ ì£¼ìš” ì´ìŠˆ: 
â€¢ ê´€ë ¨ ì¸ë¬¼/ê¸°ê´€:
â€¢ ì˜í–¥/ê²°ê³¼:

## ğŸ·ï¸ í‚¤ì›Œë“œ
(ê´€ë ¨ í‚¤ì›Œë“œ 3-5ê°œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„)

## ğŸ“Š ì¢…í•© í‰ê°€
(ì „ë°˜ì ì¸ ìƒí™© í‰ê°€ì™€ í–¥í›„ ì „ë§ 1-2ë¬¸ì¥)

ëª¨ë“  ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì¶œì²˜ ì •ë³´ëŠ” ë³„ë„ë¡œ ì œê³µë˜ë¯€ë¡œ ìš”ì•½ ë³¸ë¬¸ì—ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

def get_analysis_summary_prompt():
    """ë¶„ì„ ì¤‘ì‹¬ ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸"""
    return """ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{query}' ì£¼ì œì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ê¸°ì‚¬ë“¤:
{articles}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

## ğŸ¯ í•µì‹¬ ì´ìŠˆ ë¶„ì„
(ê°€ì¥ ì¤‘ìš”í•œ ì´ìŠˆì™€ ê·¸ ë°°ê²½)

## ğŸ“ˆ í˜„í™© ë° íŠ¸ë Œë“œ
â€¢ í˜„ì¬ ìƒí™©:
â€¢ ë³€í™” ì¶”ì´:
â€¢ ì£¼ëª©í•  ì :

## âš¡ ì£¼ìš” ë™í–¥
â€¢ ê¸ì •ì  ìš”ì†Œ:
â€¢ ìš°ë ¤ì‚¬í•­:
â€¢ ì˜ˆìƒ ì‹œë‚˜ë¦¬ì˜¤:

## ğŸŒŸ ì‹œì‚¬ì  ë° ì „ë§
(ì´ ë‰´ìŠ¤ê°€ ê°–ëŠ” ì˜ë¯¸ì™€ í–¥í›„ ì˜ˆìƒë˜ëŠ” ë°œì „ ë°©í–¥)

## ğŸ·ï¸ í•µì‹¬ í‚¤ì›Œë“œ
(ë¶„ì„ì— ì¤‘ìš”í•œ í‚¤ì›Œë“œ 5-7ê°œ)

ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ ì‹œê°ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì¶œì²˜ ì •ë³´ëŠ” ë³„ë„ë¡œ ì œê³µë˜ë¯€ë¡œ ë¶„ì„ ë³¸ë¬¸ì—ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

@router.post("/news/analysis")
async def analyze_news_trends(request: NewsAnalysisRequest):
    """ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ íŠ¸ë Œë“œë¥¼ íŒŒì•…í•©ë‹ˆë‹¤ (ìŠ¤íŠ¸ë¦¬ë°)"""
    
    async def generate_analysis_stream():
        try:
            from utils.helpers import search_news
            
            # ì‹œì‘ ì‹ í˜¸
            yield f"data: {json.dumps({'status': 'starting', 'message': 'ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...'}, ensure_ascii=False)}\n\n"
            
            # LLM í•¸ë“¤ëŸ¬ ê°€ì ¸ì˜¤ê¸°
            handler = get_llm_handler(request.model_key)
            
            logger.info(f"ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ìš”ì²­ - ì¹´í…Œê³ ë¦¬: {request.categories}")
            logger.debug(f"LLM í•¸ë“¤ëŸ¬ ë¡œë“œ ì™„ë£Œ: {handler.model_key}")
            
            # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì„¤ì •
            categories = request.categories if request.categories else ['politics', 'economy', 'technology', 'society']
            logger.debug(f"ë¶„ì„í•  ì¹´í…Œê³ ë¦¬: {categories}")
            
            categories_text = ', '.join(categories)
            yield f"data: {json.dumps({'status': 'categories', 'message': f'ë¶„ì„í•  ì¹´í…Œê³ ë¦¬: {categories_text}'}, ensure_ascii=False)}\n\n"
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
            all_news = []
            category_summaries = {}
            
            for i, category in enumerate(categories):
                search_message = f'{category} ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘... ({i+1}/{len(categories)})'
                yield f"data: {json.dumps({'status': 'searching', 'message': search_message}, ensure_ascii=False)}\n\n"
                
                logger.debug(f"'{category}' ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘...")
                category_news = search_news(
                    "ìµœì‹  ë‰´ìŠ¤", 
                    max_results=request.max_results//len(categories), 
                    category=category,
                    time_range=request.time_range
                )
                logger.debug(f"'{category}' ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ {len(category_news) if category_news else 0}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                
                if category_news:
                    all_news.extend(category_news)
                    
                    category_count = len(category_news)
                    analyzing_message = f'{category} ì¹´í…Œê³ ë¦¬ ({category_count}ê°œ ê¸°ì‚¬) ë¶„ì„ ì¤‘...'
                    yield f"data: {json.dumps({'status': 'category_analyzing', 'message': analyzing_message}, ensure_ascii=False)}\n\n"
                    
                    # ì¹´í…Œê³ ë¦¬ë³„ ê°„ë‹¨ ìš”ì•½
                    category_text = "\n".join([
                        f"â€¢ {news.get('title', '')}: {news.get('content', '')[:200]}"
                        for news in category_news[:3]
                    ])
                    
                    category_prompt = f"ë‹¤ìŒ {category} ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ë“¤ì˜ ì£¼ìš” íŠ¸ë Œë“œë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n{category_text}"
                    category_summary = handler.generate(category_prompt, max_length=256)
                    category_summaries[category] = category_summary
                    
                    yield f"data: {json.dumps({'status': 'category_completed', 'category': category, 'summary': category_summary}, ensure_ascii=False)}\n\n"
            
            total_news = len(all_news)
            overall_message = f'ì´ {total_news}ê°œ ê¸°ì‚¬ë¡œ ì „ì²´ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...'
            yield f"data: {json.dumps({'status': 'overall_analyzing', 'message': overall_message}, ensure_ascii=False)}\n\n"
            
            # ì „ì²´ íŠ¸ë Œë“œ ë¶„ì„ í”„ë¡¬í”„íŠ¸
            trend_analysis_prompt = """ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ë“¤ê³¼ ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ì œëª©ë“¤:
{titles}

ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½:
{category_summaries}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

## ğŸ”¥ ì˜¤ëŠ˜ì˜ ì£¼ìš” íŠ¸ë Œë“œ
(ê°€ì¥ ì£¼ëª©ë°›ëŠ” ì´ìŠˆ 2-3ê°œ)

## ğŸ“Š ë¶„ì•¼ë³„ ë™í–¥
â€¢ ì •ì¹˜: (ì •ì¹˜ ê´€ë ¨ ì£¼ìš” ì´ìŠˆ)
â€¢ ê²½ì œ: (ê²½ì œ ê´€ë ¨ ì£¼ìš” ì´ìŠˆ)  
â€¢ ì‚¬íšŒ: (ì‚¬íšŒ ê´€ë ¨ ì£¼ìš” ì´ìŠˆ)
â€¢ ê¸°ìˆ : (ê¸°ìˆ  ê´€ë ¨ ì£¼ìš” ì´ìŠˆ)

## ğŸ­ ì—¬ë¡  ë° ê´€ì‹¬ë„
(êµ­ë¯¼ë“¤ì´ ê°€ì¥ ê´€ì‹¬ ê°–ëŠ” ì´ìŠˆë“¤ê³¼ ì—¬ë¡ ì˜ ë°©í–¥)

## ğŸ”® ì£¼ëª©í•  í¬ì¸íŠ¸
(ì•ìœ¼ë¡œ ê³„ì† ì£¼ëª©í•´ì•¼ í•  ì´ìŠˆë“¤)

ê°ê´€ì ì´ê³  ê· í˜•ì¡íŒ ì‹œê°ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ì¶œì²˜ ì •ë³´ëŠ” ë³„ë„ë¡œ ì œê³µë˜ë¯€ë¡œ ë¶„ì„ ë³¸ë¬¸ì—ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""
            
            all_titles = [news.get('title', '') for news in all_news]
            titles_text = "\n".join([f"â€¢ {title}" for title in all_titles[:30]])
            
            full_trend_prompt = trend_analysis_prompt.format(
                titles=titles_text,
                category_summaries="\n".join([f"{cat}: {summary}" for cat, summary in category_summaries.items()])
            )
            
            logger.info("ì „ì²´ íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•œ LLM ìƒì„± ì¤‘...")
            
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ë¶„ì„ ìƒì„±
            trend_stream = handler.generate(full_trend_prompt, max_length=1024, stream=True)
            
            analysis_parts = []
            for chunk in trend_stream:
                if chunk:
                    analysis_parts.append(chunk)
                    yield f"data: {json.dumps({'status': 'streaming', 'chunk': chunk}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.01)
            
            trend_response = ''.join(analysis_parts)
            logger.debug("ì „ì²´ íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ")
            
            # ë¶„ì„ì— ì‚¬ìš©ëœ ì£¼ìš” ë‰´ìŠ¤ ì¶œì²˜ ì •ë³´ ì¤€ë¹„
            top_articles = sorted(all_news, key=lambda x: x.get('score', 0), reverse=True)[:10]
            source_info = "\n\nğŸ“° **ë¶„ì„ ê¸°ë°˜ ì£¼ìš” ë‰´ìŠ¤:**\n" + "\n".join([
                f"â€¢ [{article.get('title', 'Unknown')}]({article.get('url', '#')})" + 
                (f" ({article.get('published_date', '')})" if article.get('published_date') else "")
                for article in top_articles[:5]
            ])
            
            trend_response_with_sources = trend_response + source_info
            
            # ì™„ë£Œ ì‹ í˜¸
            final_result = {
                "status": "completed",
                "overall_trend": trend_response_with_sources,
                "category_trends": category_summaries,
                "total_articles_analyzed": len(all_news),
                "categories": categories,
                "time_range": request.time_range,
                "analyzed_articles": [
                    {
                        'title': article.get('title', ''),
                        'url': article.get('url', ''),
                        'category': article.get('category', 'general'),
                        'published_date': article.get('published_date', ''),
                        'score': article.get('score', 0)
                    } for article in top_articles[:10]
                ],
                "model_info": {
                    "model_key": handler.model_key,
                    "model_id": handler.SUPPORTED_MODELS[handler.model_key]["model_id"]
                }
            }
            
            yield f"data: {json.dumps(final_result, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {e}")
            analysis_error_msg = f'ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}'
            yield f"data: {json.dumps({'status': 'error', 'message': analysis_error_msg}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(generate_analysis_stream(), media_type="text/plain; charset=utf-8")

@router.get("/news/search")
async def search_news_endpoint(
    query: str,
    max_results: int = 5,
    category: str = None,
    time_range: str = 'd'
):
    """íŠ¹ì • í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    try:
        from utils.helpers import search_news
        
        logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ìš”ì²­ - ì¿¼ë¦¬: {query}, ì¹´í…Œê³ ë¦¬: {category}")
        
        news_results = search_news(
            query=query,
            max_results=max_results,
            category=category,
            time_range=time_range
        )
        
        return {
            "news": news_results,
            "total_count": len(news_results),
            "query": query,
            "category": category,
            "time_range": time_range,
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

@router.get("/news/categories")
async def get_news_categories():
    """ì§€ì›ë˜ëŠ” ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤"""
    categories = {
        "politics": "ì •ì¹˜",
        "economy": "ê²½ì œ", 
        "technology": "ê¸°ìˆ /IT",
        "sports": "ìŠ¤í¬ì¸ ",
        "health": "ê±´ê°•/ì˜ë£Œ",
        "culture": "ë¬¸í™”/ì˜ˆìˆ ",
        "society": "ì‚¬íšŒ",
        "international": "êµ­ì œ/í•´ì™¸"
    }
    
    return {
        "categories": categories,
        "supported_time_ranges": {
            "d": "1ì¼",
            "w": "1ì£¼", 
            "m": "1ë‹¬"
        },
        "supported_summary_types": {
            "brief": "ê°„ë‹¨ ìš”ì•½",
            "comprehensive": "í¬ê´„ì  ìš”ì•½",
            "analysis": "ì‹¬ì¸µ ë¶„ì„"
        },
        "status": "success"
    }

# === External-Web RAG API ì—”ë“œí¬ì¸íŠ¸ë“¤ ===

@router.post("/external-web/upload-topic")
async def external_web_upload_topic(request: ExternalWebUploadRequest):
    """ì™¸ë¶€ ì›¹ ê²€ìƒ‰ì„ í†µí•´ íŠ¹ì • ì£¼ì œì˜ ì •ë³´ë¥¼ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤"""
    try:
        service = get_rag_service()
        added_chunks, message = service.add_documents_from_web(request.topic, request.max_results)
        
        if added_chunks > 0:
            return {
                "success": True,
                "message": message,
                "topic": request.topic,
                "added_chunks": added_chunks,
                "max_results": request.max_results
            }
        else:
            raise HTTPException(status_code=404, detail=message)
            
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"External-Web ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"External-Web ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

@router.post("/external-web/rag-query")
async def external_web_rag_query(request: ExternalWebQueryRequest):
    """ì™¸ë¶€ ì›¹ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ RAG ì§ˆì˜ì‘ë‹µ"""
    try:
        service = get_rag_service(request.model_key)
        
        # ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = service.get_relevant_documents(request.prompt, request.top_k)
        
        if not relevant_docs:
            return {
                "response": "ì™¸ë¶€ ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ì¶©ë¶„í•œ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¨¼ì € ê´€ë ¨ ì£¼ì œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
                "prompt": request.prompt,
                "relevant_documents": [],
                "source": "external-web"
            }
        
        # RAG ì‘ë‹µ ìƒì„±
        response = service.generate_response(request.prompt)
        
        return {
            "response": response,
            "prompt": request.prompt,
            "relevant_documents": relevant_docs[:request.top_k],
            "source": "external-web",
            "model_info": {
                "model_key": service.llm_handler.model_key,
                "model_id": service.llm_handler.SUPPORTED_MODELS[service.llm_handler.model_key]["model_id"],
                "description": service.llm_handler.SUPPORTED_MODELS[service.llm_handler.model_key]["description"],
                "category": service.llm_handler.SUPPORTED_MODELS[service.llm_handler.model_key]["category"],
                "loaded": service.llm_handler.model is not None
            }
        }
        
    except Exception as e:
        logger.error(f"External-Web RAG ì§ˆì˜ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"External-Web RAG ì§ˆì˜ ì‹¤íŒ¨: {str(e)}")

# === Internal-DBMS RAG API ì—”ë“œí¬ì¸íŠ¸ë“¤ ===

@router.get("/internal-db/tables")
async def get_internal_db_tables():
    """ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì˜ í…Œì´ë¸” ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤"""
    try:
        service = get_internal_db_service()
        tables = await service.get_db_tables()
        
        return {
            "tables": tables,
            "total_count": len(tables),
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"DB í…Œì´ë¸” ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"DB í…Œì´ë¸” ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.post("/internal-db/ingest")
async def internal_db_ingest(request: InternalDBIngestRequest):
    """ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì„ ë²¡í„°í™”í•˜ì—¬ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
    try:
        service = get_internal_db_service()
        result = await service.ingest_table(
            table_name=request.table,
            save_name=request.save_name,
            simulate=request.simulate,
            id_col=request.id_col,
            title_col=request.title_col,
            text_cols=request.text_cols
        )
        
        return {
            "ok": True,
            "save_dir": result["save_dir"],
            "rows": result["rows"],
            "chunks": result["chunks"],
            "schema": result["schema"],
            "table": request.table,
            "simulate": request.simulate
        }
        
    except Exception as e:
        logger.error(f"Internal DB ì¸ì œìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"Internal DB ì¸ì œìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

@router.post("/internal-db/query")
async def internal_db_query(request: InternalDBQueryRequest):
    """ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ RAG ì§ˆì˜ì‘ë‹µ"""
    try:
        service = get_internal_db_service()
        result = await service.query(
            save_name=request.save_name,
            question=request.question,
            top_k=request.top_k,
            margin=request.margin
        )
        
        return {
            "answer": result["answer"],
            "sources": result["sources"],
            "question": request.question,
            "save_name": request.save_name,
            "top_k": request.top_k,
            "margin": request.margin
        }
        
    except Exception as e:
        logger.error(f"Internal DB ì§ˆì˜ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"Internal DB ì§ˆì˜ ì‹¤íŒ¨: {str(e)}")

@router.get("/internal-db/status")
async def get_internal_db_status():
    """ë‚´ë¶€ DB FAISS ì¸ë±ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤"""
    try:
        service = get_internal_db_service()
        status = await service.get_status()
        
        return {
            "faiss_indices": status["faiss_indices"],
            "cache_keys": status["cache_keys"],
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"Internal DB ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"Internal DB ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")