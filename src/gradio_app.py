import gradio as gr
import requests
import json

# FastAPI ì„œë²„ ì£¼ì†Œ
API_URL = "http://127.0.0.1:8000/api/v1"

def handle_api_error(response):
    """API ì‘ë‹µ ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ê³  ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if response.status_code == 500:
        try:
            detail = response.json().get("detail", "ì•Œ ìˆ˜ ì—†ëŠ” ì„œë²„ ì˜¤ë¥˜")
            return f"ì˜¤ë¥˜: ë°±ì—”ë“œ ì„œë²„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. \n{detail}"
        except json.JSONDecodeError:
            return f"ì˜¤ë¥˜: ì„œë²„ì—ì„œ ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ë‹µì„ ë³´ëƒˆìŠµë‹ˆë‹¤. \në‚´ìš©: {response.text}"
    response.raise_for_status() # 500 ì™¸ ë‹¤ë¥¸ HTTP ì—ëŸ¬ ì²˜ë¦¬
    return None

def generate_text(prompt, model_key):
    """'/generate' ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if not prompt:
        return "ì˜¤ë¥˜: í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", ""
    try:
        response = requests.post(
            f"{API_URL}/generate",
            json={"prompt": prompt, "model_key": model_key or None}
        )
        error = handle_api_error(response)
        if error:
            return error, ""
        
        data = response.json()
        return data.get("response", "ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤."), data.get("model_info", "")
    except requests.exceptions.RequestException as e:
        return f"ì˜¤ë¥˜: ë°±ì—”ë“œ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.\n{e}", ""

def chat_with_bot(message, history, model_key):
    """'/chat' ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•˜ì—¬ ì±„íŒ… ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤."""
    try:
        response = requests.post(
            f"{API_URL}/chat",
            json={"message": message, "model_key": model_key or None}
        )
        error = handle_api_error(response)
        if error:
            return error
        
        return response.json().get("response", "ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
    except requests.exceptions.RequestException as e:
        return f"ì˜¤ë¥˜: ë°±ì—”ë“œ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.\n{e}"

def rag_query(question, model_key):
    """'/rag' ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•˜ì—¬ RAG ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤."""
    if not question:
        return "ì˜¤ë¥˜: ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "", ""
    try:
        response = requests.post(
            f"{API_URL}/rag",
            json={"question": question, "model_key": model_key or None}
        )
        error = handle_api_error(response)
        if error:
            return error, "", ""
            
        data = response.json()
        answer = data.get("response", "ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
        docs = data.get("relevant_documents", [])
        
        # ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
        doc_str = ""
        if docs:
            for doc in docs:
                doc_str += f"### ğŸ“„ {doc.get('title', 'ì¶œì²˜ ì—†ìŒ')}\n"
                doc_str += f"{doc.get('content', '')}\n\n---\n\n"
        else:
            doc_str = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            
        model_info = data.get("model_info", "")
        return answer, doc_str, model_info
    except requests.exceptions.RequestException as e:
        return f"ì˜¤ë¥˜: ë°±ì—”ë“œ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.\n{e}", "", ""

def get_available_models():
    """ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        response = requests.get(f"{API_URL}/models")
        response.raise_for_status()
        return list(response.json().get("supported_models", {}).keys())
    except requests.exceptions.RequestException:
        return []

# UI êµ¬ì„±
with gr.Blocks(theme=gr.themes.Soft()) as gradio_ui:
    gr.Markdown("# ğŸ¤– LLM FastAPI ì„œë²„ UI")
    gr.Markdown("Gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ì„œë²„ì˜ LLM ê¸°ëŠ¥ì„ ì‰½ê²Œ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
    
    model_choices = get_available_models()
    
    with gr.Tabs():
        # 1. í…ìŠ¤íŠ¸ ìƒì„± íƒ­
        with gr.TabItem("ğŸ“ í…ìŠ¤íŠ¸ ìƒì„±"):
            with gr.Row():
                with gr.Column(scale=2):
                    gen_prompt = gr.Textbox(lines=5, label="í”„ë¡¬í”„íŠ¸", placeholder="ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì§§ì€ ê¸€ì„ ì¨ì¤˜.")
                    gen_model_select = gr.Dropdown(choices=["ê¸°ë³¸ ëª¨ë¸"] + model_choices, value="ê¸°ë³¸ ëª¨ë¸", label="ëª¨ë¸ ì„ íƒ")
                    gen_button = gr.Button("ìƒì„±í•˜ê¸°", variant="primary")
                with gr.Column(scale=3):
                    gen_output = gr.Textbox(lines=10, label="ìƒì„±ëœ í…ìŠ¤íŠ¸", interactive=False)
                    gen_model_info = gr.JSON(label="ì‚¬ìš©ëœ ëª¨ë¸ ì •ë³´")

        # 2. ì±„íŒ… íƒ­
        with gr.TabItem("ğŸ’¬ ì±„íŒ…"):
            with gr.Row():
                chat_model_select = gr.Dropdown(choices=["ê¸°ë³¸ ëª¨ë¸"] + model_choices, value="ê¸°ë³¸ ëª¨ë¸", label="ì±„íŒ… ëª¨ë¸ ì„ íƒ")
            
            gr.ChatInterface(
                fn=lambda message, history: chat_with_bot(message, history, chat_model_select.value),
                chatbot=gr.Chatbot(height=400, label="ì±„íŒ…ì°½", bubble_full_width=False),
                textbox=gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", label="ì…ë ¥"),
                submit_btn="ë³´ë‚´ê¸°"
            )

        # 3. RAG ì§ˆì˜ì‘ë‹µ íƒ­
        with gr.TabItem("ğŸ“š RAG ì§ˆì˜ì‘ë‹µ"):
            with gr.Row():
                with gr.Column(scale=2):
                    rag_question = gr.Textbox(lines=2, label="ì§ˆë¬¸", placeholder="íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì´ì•¼?")
                    rag_model_select = gr.Dropdown(choices=["ê¸°ë³¸ ëª¨ë¸"] + model_choices, value="ê¸°ë³¸ ëª¨ë¸", label="ëª¨ë¸ ì„ íƒ")
                    rag_button = gr.Button("ì§ˆë¬¸í•˜ê¸°", variant="primary")
                with gr.Column(scale=3):
                    rag_answer = gr.Textbox(lines=5, label="ë‹µë³€", interactive=False)
                    rag_model_info_output = gr.JSON(label="ì‚¬ìš©ëœ ëª¨ë¸ ì •ë³´")
            
            with gr.Row():
                rag_docs = gr.Markdown(label="ì°¸ê³  ë¬¸ì„œ")

    # ë²„íŠ¼ê³¼ í•¨ìˆ˜ ì—°ê²°
    gen_button.click(
        fn=generate_text,
        inputs=[gen_prompt, gen_model_select],
        outputs=[gen_output, gen_model_info]
    )
    rag_button.click(
        fn=rag_query,
        inputs=[rag_question, rag_model_select],
        outputs=[rag_answer, rag_docs, rag_model_info_output]
    )
