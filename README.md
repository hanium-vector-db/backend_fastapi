# LLM FastAPI ì„œë²„

Retrieval-Augmented Generation (RAG) ê¸°ëŠ¥ê³¼ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì„ ê°–ì¶˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) ë°°í¬ë¥¼ ìœ„í•œ í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ FastAPI ì„œë²„ì…ë‹ˆë‹¤. 3ê°œì˜ ê³ ì„±ëŠ¥ ëª¨ë¸ê³¼ ë‹¤ì–‘í•œ AI ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ì£¼ìš” ê¸°ëŠ¥

- **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: Server-Sent Eventsë¥¼ í†µí•œ í† í°ë³„ ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ìƒì„±
- **3ê°œ ê³ ì„±ëŠ¥ LLM ëª¨ë¸**: 4bit ì–‘ìí™”ë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ìµœì í™”
- **ì´ì¤‘ UI ì§€ì›**: Gradio UI + ì „ìš© ìŠ¤íŠ¸ë¦¬ë° ì›¹ í˜ì´ì§€
- **RAG (ê²€ìƒ‰ ì¦ê°• ìƒì„±)**: ì§€ëŠ¥ì ì¸ ë¬¸ì„œ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì‘ë‹µ
- **Tavily ë‰´ìŠ¤ ê¸°ëŠ¥**: ì‹¤ì‹œê°„ ë‰´ìŠ¤ ê²€ìƒ‰, AI ìš”ì•½, íŠ¸ë Œë“œ ë¶„ì„
- **ì„ë² ë”© ìƒì„±**: BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
- **ì±„íŒ… ì¸í„°í˜ì´ìŠ¤**: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ëŠ” ëŒ€í™”í˜• ì±„íŒ… ê¸°ëŠ¥
- **RESTful API**: ìë™ OpenAPI ë¬¸ì„œí™”ê°€ í¬í•¨ëœ ì˜ ë¬¸ì„œí™”ëœ API ì—”ë“œí¬ì¸íŠ¸
- **Docker ì§€ì›**: ì‰¬ìš´ í™•ì¥ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆí™”ëœ ë°°í¬

## ì‹¤í–‰ í™˜ê²½ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **CPU**: Intel/AMD 64-bit í”„ë¡œì„¸ì„œ (8ì½”ì–´ ì´ìƒ ê¶Œì¥)
- **RAM**: ìµœì†Œ 16GB (32GB ì´ìƒ ê¶Œì¥)
- **GPU**: NVIDIA GPU with CUDA support
  - ìµœì†Œ 8GB VRAM (RTX 3070, RTX 4060 Ti ì´ìƒ)
  - ê¶Œì¥ 12GB VRAM (RTX 3080, RTX 4070 Ti ì´ìƒ)
- **ì €ì¥ê³µê°„**: 50GB ì´ìƒ (ëª¨ë¸ íŒŒì¼ í¬í•¨)

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **ìš´ì˜ì²´ì œ**: Linux (Ubuntu 20.04+ ê¶Œì¥)
- **Python**: 3.11.x (í•„ìˆ˜)
- **CUDA**: 12.1 ì´ìƒ

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
llm-fastapi-server/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                    # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì§„ì…ì  (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
â”‚   â”œâ”€â”€ gradio_app.py              # Gradio UI ì¸í„°í˜ì´ìŠ¤
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes.py              # API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ (ìŠ¤íŠ¸ë¦¬ë° API í¬í•¨)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ llm_handler.py         # LLM ëª¨ë¸ ê´€ë¦¬ (ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥)
â”‚   â”‚   â””â”€â”€ embedding_handler.py   # ì„ë² ë”© ëª¨ë¸ ê´€ë¦¬
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ rag_service.py         # RAG ê¸°ëŠ¥
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py              # ì„¤ì • ê´€ë¦¬
â”‚   â”‚   â””â”€â”€ logger.py              # ë¡œê¹… ì„¤ì •
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ helpers.py             # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ static/
â”‚   â””â”€â”€ streaming.html             # ì „ìš© ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í˜ì´ì§€
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vector_db/                 # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì†Œ
â”œâ”€â”€ test_qwen.py                           # ë‹¨ë… ëª¨ë¸ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ test_api.py                            # API ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ test_streaming.py                      # ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ requirements.txt                       # Python ì˜ì¡´ì„±
â”œâ”€â”€ environment_python311_llm_server.yml   # Conda í™˜ê²½ íŒŒì¼ (Python 3.11)
â”œâ”€â”€ Dockerfile                             # Docker ì„¤ì •
â”œâ”€â”€ docker-compose.yml                     # Docker Compose ì„¤ì •
â”œâ”€â”€ API.md                                 # API ëª…ì„¸ì„œ
â””â”€â”€ README.md                              # í”„ë¡œì íŠ¸ ë¬¸ì„œ
```

## ğŸ”§ ì„¤ì¹˜


### ì„¤ì •

1. **ì €ì¥ì†Œ ë³µì œ**
   ```bash
   git clone https://github.com/hanium-vector-db/AWS_LOCAL_LLM.git
   cd AWS_LOCAL_LLM
   ```

2. **ê°€ìƒ í™˜ê²½ ìƒì„±**
   ```bash
   python -m venv venv
   source venv/bin/activate 
   ```

3. **í™˜ê²½ ì„¤ì •**
   
   **ë°©ë²• 1: Conda í™˜ê²½ ë³µì› (ê¶Œì¥)**
   ```bash
   # ì œê³µëœ í™˜ê²½ íŒŒì¼ë¡œ ì™„ì „í•œ í™˜ê²½ ë³µì›
   conda env create -f environment_python311_llm_server.yml
   conda activate llm_server
   ```
   
   **ë°©ë²• 2: ìˆ˜ë™ ì„¤ì¹˜**
   ```bash
   # ì˜ì¡´ì„± ì„¤ì¹˜
   pip install -r requirements.txt
   ```

4. **Hugging Face í† í° ì„¤ì •**
   ```bash
   # í™˜ê²½ ë³€ìˆ˜ë¡œ Hugging Face í† í° ì„¤ì •
   export HUGGINGFACE_TOKEN="your_token_here"
   ```

## ğŸš€ ì‚¬ìš©ë²•

### ì„œë²„ ì‹¤í–‰

#### ë°©ë²• 1: Python ì§ì ‘ ì‹¤í–‰ (ê¶Œì¥)
```bash
# ì•„ë‚˜ì½˜ë‹¤ í™˜ê²½ í™œì„±í™” (ê¶Œì¥)
conda activate llm_server

# ì„œë²„ ì‹œì‘
python src/main.py
```

#### ë°©ë²• 2: uvicorn ì‚¬ìš©
```bash
uvicorn src.main:app --host 0.0.0.0 --port 8001 --reload
```

#### ë°©ë²• 3: Docker ì‚¬ìš©
```bash
# Docker Composeë¡œ ë¹Œë“œ ë° ì‹¤í–‰
docker-compose up -d
```

### ğŸ“± ì›¹ ì¸í„°í˜ì´ìŠ¤ ì ‘ê·¼

ì„œë²„ê°€ ì‹œì‘ë˜ë©´ ë‹¤ìŒ ì›¹ ì¸í„°í˜ì´ìŠ¤ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ğŸ”¥ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í˜ì´ì§€**: `http://localhost:8001/stream` (ì¶”ì²œ!)
- **Gradio UI**: `http://localhost:8001/ui`
- **API ë¬¸ì„œ (Swagger)**: `http://localhost:8001/docs`
- **ReDoc**: `http://localhost:8001/redoc`

### ğŸ¯ ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ ì‚¬ìš©ë²•

#### ğŸ”¥ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í˜ì´ì§€ (ì¶”ì²œ)
1. ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:8001/stream` ì ‘ì†
2. í”„ë¡¬í”„íŠ¸ ì…ë ¥
3. ëª¨ë¸ ì„ íƒ (ì„ íƒì‚¬í•­)
4. "ìƒì„±í•˜ê¸°" ë²„íŠ¼ í´ë¦­
5. ì‹¤ì‹œê°„ìœ¼ë¡œ í† í°ë³„ í…ìŠ¤íŠ¸ ìƒì„± í™•ì¸!

#### ğŸ“‹ API ìŠ¤íŠ¸ë¦¬ë°
```bash
# ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ í…ìŠ¤íŠ¸ ìƒì„±
curl -X POST "http://localhost:8001/api/v1/generate" \
     -H "Content-Type: application/json" \
     -H "Accept: text/event-stream" \
     -d '{"prompt": "Pythonì˜ ì¥ì ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”", "stream": true}'
```

## API ì—”ë“œí¬ì¸íŠ¸

### ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸

| ë©”ì†Œë“œ | ì—”ë“œí¬ì¸íŠ¸ | ì„¤ëª… | ìŠ¤íŠ¸ë¦¬ë° ì§€ì› |
|--------|-----------|------|-------------|
| GET | `/` | í™˜ì˜ ë©”ì‹œì§€ ë° ì„œë¹„ìŠ¤ ê°œìš” | - |
| GET | `/stream` | ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì›¹ í˜ì´ì§€ | âœ… |
| GET | `/ui` | Gradio í†µí•© ì¸í„°í˜ì´ìŠ¤ | - |
| GET | `/api/v1/health` | ì„œë²„ ìƒíƒœ ë° ëª¨ë¸ ì •ë³´ í™•ì¸ | - |

### í…ìŠ¤íŠ¸ ìƒì„±

#### POST `/api/v1/generate`
LLMì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)

**ìš”ì²­ ë³¸ë¬¸:**
```json
{
    "prompt": "Pythonì˜ ì¥ì ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "max_length": 512,
    "model_key": "qwen2.5-7b",
    "stream": true
}
```

**ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°):**
```
data: {"content": "Pythonì€", "done": false}
data: {"content": " ê°„ê²°í•˜ê³ ", "done": false}
data: {"content": "", "done": true}
```

**ì‘ë‹µ (ì¼ë°˜):**
```json
{
    "response": "Pythonì€ ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ì„ ê°€ì§„ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤...",
    "prompt": "Pythonì˜ ì¥ì ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "model_info": {
        "model_key": "qwen2.5-7b",
        "model_id": "Qwen/Qwen2.5-7B-Instruct",
        "description": "Qwen 2.5 7B - ê³ ì„±ëŠ¥ ë²”ìš© ëª¨ë¸"
    }
}
```

#### POST `/api/v1/chat`
ëŒ€í™”í˜• ì±„íŒ… (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)

**ìš”ì²­ ë³¸ë¬¸:**
```json
{
    "message": "ì•ˆë…•í•˜ì„¸ìš”!",
    "model_key": "llama3.1-8b",
    "stream": true
}
```

### ì„ë² ë”© ìƒì„±

#### POST `/api/v1/embed`
í…ìŠ¤íŠ¸ ì„ë² ë”© ë²¡í„° ìƒì„±

**ìš”ì²­ ë³¸ë¬¸:**
```json
{
    "text": "ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•  í…ìŠ¤íŠ¸"
}
```

**ì‘ë‹µ:**
```json
{
    "embedding": [0.1, -0.2, 0.3, ...],
    "dimension": 1024,
    "model_info": {
        "model_name": "BAAI/bge-m3",
        "embedding_dimension": 1024
    }
}
```

### RAG (ê²€ìƒ‰ ì¦ê°• ìƒì„±)

#### POST `/api/v1/rag`
ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ

**ìš”ì²­ ë³¸ë¬¸:**
```json
{
    "question": "AI ê¸°ìˆ ì˜ ìµœì‹  ë™í–¥ì€?",
    "model_key": "qwen2.5-7b"
}
```

#### POST `/api/v1/rag/update-news`
ìµœì‹  ë‰´ìŠ¤ë¡œ RAG ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸

### ë‰´ìŠ¤ ê¸°ëŠ¥

#### GET `/api/v1/news/latest`
ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ

**ì¿¼ë¦¬ ë§¤ê°œë³€ìˆ˜:**
- `categories`: ì¹´í…Œê³ ë¦¬ (ì‰¼í‘œë¡œ êµ¬ë¶„) - `technology,economy,politics`
- `max_results`: ìµœëŒ€ ê²°ê³¼ ìˆ˜ (1-20)
- `time_range`: ì‹œê°„ ë²”ìœ„ (`d`, `w`, `m`)

#### GET `/api/v1/news/search`
í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰

#### POST `/api/v1/news/summary`
AI ë‰´ìŠ¤ ìš”ì•½ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)

#### POST `/api/v1/news/analysis`
ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)

#### GET `/api/v1/models`
ì§€ì›í•˜ëŠ” ëª¨ë¸ ëª©ë¡ ì¡°íšŒ

#### POST `/api/v1/models/switch`
í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì „í™˜

#### POST `/api/v1/models/recommend`
ì‹œìŠ¤í…œ ì‚¬ì–‘ ë§ì¶¤ ëª¨ë¸ ì¶”ì²œ

### ì‹œìŠ¤í…œ ì •ë³´

#### GET `/api/v1/system/gpu`
GPU ë©”ëª¨ë¦¬ ë° ì‚¬ìš©ëŸ‰ ì •ë³´

**ì‘ë‹µ:**
```json
{
    "gpu_available": true,
    "gpu_count": 1,
    "gpu_memory": {
        "total": 12288,
        "used": 8192,
        "free": 4096
    },
    "gpu_utilization": 65.5,
    "cuda_version": "12.1"
}
```

## ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜

| ë³€ìˆ˜ëª… | ì„¤ëª… | ê¸°ë³¸ê°’ | í•„ìˆ˜ ì—¬ë¶€ |
|--------|------|--------|----------|
| `HUGGINGFACE_TOKEN` | Hugging Face API í† í° | - | í•„ìˆ˜ |
| `TAVILY_API_KEY` | Tavily ë‰´ìŠ¤ ê²€ìƒ‰ API í‚¤ | - | ì„ íƒ (ë‰´ìŠ¤ ê¸°ëŠ¥ìš©) |
| `MODEL_ID` | ê¸°ë³¸ LLM ëª¨ë¸ ì‹ë³„ì | `qwen2.5-7b` | ì„ íƒ |
| `EMBEDDING_MODEL` | ì„ë² ë”© ëª¨ë¸ ì´ë¦„ | `BAAI/bge-m3` | ì„ íƒ |
| `CUDA_VISIBLE_DEVICES` | ì‚¬ìš©í•  GPU ë””ë°”ì´ìŠ¤ | `0` | ì„ íƒ |

### ëª¨ë¸ ì €ì¥ ìœ„ì¹˜

ëª¨ë“  Hugging Face ëª¨ë¸ì€ `~/.huggingface_models/`ì— ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë° ìºì‹œë©ë‹ˆë‹¤.

### GPU ì„¤ì •

ì‹œìŠ¤í…œì€ ë‹¨ì¼ GPU (CUDA:0) ì‚¬ìš©ìœ¼ë¡œ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
- `CUDA_VISIBLE_DEVICES=0` í™˜ê²½ë³€ìˆ˜ë¡œ GPU 0ë²ˆë§Œ ì‚¬ìš©
- ëª¨ë“  ëª¨ë¸ì´ `cuda:0` ë””ë°”ì´ìŠ¤ì— ë¡œë“œë¨
- ë³‘ë ¬ ì²˜ë¦¬ ëŒ€ì‹  ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë‹¨ì¼ GPU ì²˜ë¦¬

## ğŸ¤– ì§€ì› ëª¨ë¸ (3ê°œ)

### **qwen2.5-7b** (ê¸°ë³¸)
- **ëª¨ë¸**: Qwen/Qwen2.5-7B-Instruct
- **ì„¤ëª…**: ê³ ì„±ëŠ¥ ë²”ìš© ëª¨ë¸
- **ìš”êµ¬ì‚¬í•­**: 16GB RAM, 8GB GPU
- **íŠ¹ì§•**: í•œêµ­ì–´, ì¼ë°˜ í…ìŠ¤íŠ¸, ì½”ë”© ì§€ì›

### **llama3.1-8b**
- **ëª¨ë¸**: meta-llama/Meta-Llama-3-8B-Instruct
- **ì„¤ëª…**: Metaì˜ ê³ ì„±ëŠ¥ ëª¨ë¸
- **ìš”êµ¬ì‚¬í•­**: 16GB RAM, 8GB GPU
- **íŠ¹ì§•**: ì¶”ë¡ , ì½”ë”©, ì¼ë°˜ í…ìŠ¤íŠ¸ì— ê°•í•¨

### **gemma-3-4b**
- **ëª¨ë¸**: google/gemma-2-9b-it
- **ì„¤ëª…**: Googleì˜ íš¨ìœ¨ì ì¸ ì¤‘í˜• ëª¨ë¸
- **ìš”êµ¬ì‚¬í•­**: 18GB RAM, 10GB GPU
- **íŠ¹ì§•**: ë‹¤êµ­ì–´ ì§€ì›, ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒì„±

## ğŸ“ API ì‚¬ìš© ì˜ˆì‹œ

### 1. ğŸ”¥ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ìƒì„±
```bash
curl -X POST "http://localhost:8001/api/v1/generate" \
     -H "Content-Type: application/json" \
     -H "Accept: text/event-stream" \
     -d '{"prompt": "Pythonì˜ ì£¼ìš” íŠ¹ì§•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.", "stream": true, "max_length": 300}'
```

### 2. ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒì„±
```bash
curl -X POST "http://localhost:8001/api/v1/generate" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì§§ì€ ê¸€ì„ ì¨ì¤˜.", "stream": false}'
```

### 3. íŠ¹ì • ëª¨ë¸ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…
```bash
curl -X POST "http://localhost:8001/api/v1/chat" \
     -H "Content-Type: application/json" \
     -H "Accept: text/event-stream" \
     -d '{"message": "ì•ˆë…•í•˜ì„¸ìš”!", "model_key": "llama3.1-8b", "stream": true}'
```

### 4. RAG ì§ˆì˜ì‘ë‹µ
```bash
curl -X POST "http://localhost:8001/api/v1/rag" \
     -H "Content-Type: application/json" \
     -d '{"question": "AI ê¸°ìˆ ì˜ ìµœì‹  ë™í–¥ì€?"}'
```

### 5. ëª¨ë¸ ì „í™˜
```bash
curl -X POST "http://localhost:8001/api/v1/models/switch" \
     -H "Content-Type: application/json" \
     -d '{"model_key": "gemma-3-4b"}'
```

### 6. ì‹œìŠ¤í…œ ìƒíƒœ ë° GPU ì •ë³´ í™•ì¸
```bash
# ì„œë²„ ìƒíƒœ í™•ì¸
curl -X GET "http://localhost:8001/api/v1/health"

# GPU ì •ë³´ í™•ì¸
curl -X GET "http://localhost:8001/api/v1/system/gpu"

# ëª¨ë¸ ê²€ìƒ‰
curl -X GET "http://localhost:8001/api/v1/models/search?keyword=korean"

# ëª¨ë¸ í†µê³„
curl -X GET "http://localhost:8001/api/v1/models/stats"
```

### ğŸ†• 7. ë‰´ìŠ¤ ê¸°ëŠ¥ ì‚¬ìš© ì˜ˆì‹œ
```bash
# ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ
curl -X GET "http://localhost:8001/api/v1/news/latest?categories=technology,economy&max_results=5"

# ë‰´ìŠ¤ ê²€ìƒ‰
curl -X GET "http://localhost:8001/api/v1/news/search?query=ChatGPT&category=technology"

# AI ë‰´ìŠ¤ ìš”ì•½
curl -X POST "http://localhost:8001/api/v1/news/summary" \
     -H "Content-Type: application/json" \
     -d '{"query": "ì¸ê³µì§€ëŠ¥ ChatGPT", "summary_type": "comprehensive", "max_results": 5}'

# ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„
curl -X POST "http://localhost:8001/api/v1/news/analysis" \
     -H "Content-Type: application/json" \
     -d '{"categories": ["politics", "economy", "technology"], "max_results": 15}'

# ì§€ì› ì¹´í…Œê³ ë¦¬ ì¡°íšŒ
curl -X GET "http://localhost:8001/api/v1/news/categories"
```

## API ëª…ì„¸ì„œ

ì „ì²´ API ëª…ì„¸ì„œëŠ” ë‹¤ìŒ ë°©ë²•ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **API ëª…ì„¸ ë¬¸ì„œ**: [API.md](./API.md) íŒŒì¼ ì°¸ì¡°

## í…ŒìŠ¤íŠ¸

### ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
```bash
# ëª¨ë¸ ë‹¨ë… í…ŒìŠ¤íŠ¸
python debug_py/test_qwen.py

# API ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
python debug_py/test_api.py

# ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
python debug_py/test_streaming.py

# ë‰´ìŠ¤ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
python debug_py/test_news_features.py
```

### í† í° ì„¤ì • í™•ì¸
```bash
# Hugging Face í† í° í™•ì¸
python debug_py/setup_hf_token.py

# Llama ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ í™•ì¸
python debug_py/check_llama_access.py
```

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

**CUDA Out of Memory ì˜¤ë¥˜**
- GPU ë©”ëª¨ë¦¬ í™•ì¸: `nvidia-smi`
- ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© ë˜ëŠ” max_length ê°’ ê°ì†Œ

**Hugging Face í† í° ì˜¤ë¥˜**
- í† í° ì¬ì„¤ì •: `export HUGGINGFACE_TOKEN="your_new_token"`
- í† í° ê¶Œí•œ í™•ì¸: https://huggingface.co/settings/tokens

**ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨**
- ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸: `curl -I https://huggingface.co`
- ìºì‹œ ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸: `ls -la ~/.huggingface_models/`

**í¬íŠ¸ ì¶©ëŒ**
- í¬íŠ¸ ì‚¬ìš© í™•ì¸: `netstat -tlnp | grep :8001`
- ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©: `uvicorn src.main:app --port 8002`

## Docker ë°°í¬

### ê¸°ë³¸ ë°°í¬
```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t llm-fastapi-server .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name llm-server \
  --gpus all \
  -p 8001:8001 \
  -e HUGGINGFACE_TOKEN="your_token" \
  -e TAVILY_API_KEY="your_api_key" \
  -v ~/.huggingface_models:/root/.huggingface_models \
  llm-fastapi-server
```

### Docker Compose ë°°í¬
```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ í¸ì§‘

# ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d
```

## ê¸°ì—¬í•˜ê¸°

1. ì €ì¥ì†Œ í¬í¬
2. ê¸°ëŠ¥ ë¸Œëœì¹˜ ìƒì„± (`git checkout -b feature/amazing-feature`)
3. ë³€ê²½ì‚¬í•­ ì»¤ë°‹ (`git commit -m 'Add amazing feature'`)
4. ë¸Œëœì¹˜ì— í‘¸ì‹œ (`git push origin feature/amazing-feature`)
5. Pull Request ìƒì„±

## ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ì§€ì›

### ë¬¸ì„œ ë° ë„ì›€ë§
- **API ëª…ì„¸ì„œ**: [API.md](./API.md)
- **ëŒ€í™”í˜• API ë¬¸ì„œ**: http://localhost:8001/docs
- **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸**: http://localhost:8001/stream
- **Gradio UI**: http://localhost:8001/ui

### ì´ìŠˆ ë° ë²„ê·¸ ë¦¬í¬íŠ¸
GitHub Issues: https://github.com/hanium-vector-db/AWS_LOCAL_LLM/issues