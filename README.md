# LLM FastAPI ì„œë²„

Retrieval-Augmented Generation (RAG) ê¸°ëŠ¥ì„ ê°–ì¶˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM) ë°°í¬ë¥¼ ìœ„í•œ í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ FastAPI ì„œë²„ì…ë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” ë¬¸ì„œ ê²€ìƒ‰, ì„ë² ë”©, ì±„íŒ… ê¸°ëŠ¥ê³¼ ê°™ì€ ê³ ê¸‰ ê¸°ëŠ¥ìœ¼ë¡œ ì‚¬ìš©ì ì •ì˜ LLMì„ ì œê³µí•˜ëŠ” í¬ê´„ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

- **ì‚¬ìš©ì ì •ì˜ LLM ë°°í¬**: 4bit ì–‘ìí™”ë¥¼ í†µí•œ Hugging Face ëª¨ë¸ ì§€ì›
- **RAG (ê²€ìƒ‰ ì¦ê°• ìƒì„±)**: ì§€ëŠ¥ì ì¸ ë¬¸ì„œ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì‘ë‹µ
- **ì„ë² ë”© ìƒì„±**: BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
- **ì±„íŒ… ì¸í„°í˜ì´ìŠ¤**: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ëŠ” ëŒ€í™”í˜• ì±„íŒ… ê¸°ëŠ¥
- **RESTful API**: ìë™ OpenAPI ë¬¸ì„œí™”ê°€ í¬í•¨ëœ ì˜ ë¬¸ì„œí™”ëœ API ì—”ë“œí¬ì¸íŠ¸
- **Docker ì§€ì›**: ì‰¬ìš´ í™•ì¥ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆí™”ëœ ë°°í¬
- **í”„ë¡œë•ì…˜ ì¤€ë¹„**: í¬ê´„ì ì¸ ë¡œê¹…, ì˜¤ë¥˜ ì²˜ë¦¬ ë° ìƒíƒœ í™•ì¸

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
llm-fastapi-server/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                    # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì§„ì…ì 
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes.py              # API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_handler.py         # LLM ëª¨ë¸ ê´€ë¦¬
â”‚   â”‚   â””â”€â”€ embedding_handler.py   # ì„ë² ë”© ëª¨ë¸ ê´€ë¦¬
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rag_service.py         # RAG ê¸°ëŠ¥
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py              # ì„¤ì • ê´€ë¦¬
â”‚   â”‚   â””â”€â”€ logger.py              # ë¡œê¹… ì„¤ì •
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py             # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ llm_setup.ipynb            # ëª¨ë¸ ì„¤ì • ë° í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ rag_development.ipynb      # RAG ê°œë°œ ë…¸íŠ¸ë¶
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vector_db/                 # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì†Œ
â”œâ”€â”€ requirements.txt               # Python ì˜ì¡´ì„±
â”œâ”€â”€ Dockerfile                     # Docker ì„¤ì •
â”œâ”€â”€ docker-compose.yml             # Docker Compose ì„¤ì •
â””â”€â”€ README.md                      # í”„ë¡œì íŠ¸ ë¬¸ì„œ
```

## ğŸ”§ ì„¤ì¹˜

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- Python 3.11+
- CUDA í˜¸í™˜ GPU (ëŒ€í˜• ëª¨ë¸ì— ê¶Œì¥)
- Git

### ì„¤ì •

1. **ì €ì¥ì†Œ ë³µì œ**
   ```bash
   git clone https://github.com/hanium-vector-db/AWS_LOCAL_LLM.git
   cd AWS_LOCAL_LLM
   ```

2. **ê°€ìƒ í™˜ê²½ ìƒì„±**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   ```

3. **ì˜ì¡´ì„± ì„¤ì¹˜**
   ```bash
   pip install -r requirements.txt
   ```

4. **Hugging Face í† í° ì„¤ì •**
   ```bash
   # í™˜ê²½ ë³€ìˆ˜ë¡œ Hugging Face í† í° ì„¤ì •
   export HUGGINGFACE_TOKEN="your_token_here"
   ```

## ğŸš€ ì‚¬ìš©ë²•

### ì„œë²„ ì‹œì‘ ê°€ì´ë“œ

#### ğŸ”§ ì‚¬ì „ ì¤€ë¹„
ì„œë²„ë¥¼ ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”:

1. **Hugging Face í† í° ì„¤ì •** (í•„ìˆ˜)
   ```bash
   export HUGGINGFACE_TOKEN="your_hugging_face_token_here"
   # ë˜ëŠ” .env íŒŒì¼ì— ì¶”ê°€
   echo "HUGGINGFACE_TOKEN=your_token_here" >> .env
   ```

2. **GPU ë©”ëª¨ë¦¬ í™•ì¸** (ê¶Œì¥)
   ```bash
   nvidia-smi  # GPU ìƒíƒœ í™•ì¸
   ```

3. **ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸**
   ```bash
   source venv/bin/activate  # Linux/Mac
   # ë˜ëŠ” venv\Scripts\activate  # Windows
   ```

#### ğŸš€ ì„œë²„ ì‹¤í–‰ ë°©ë²•

#### ë°©ë²• 1: Python ì§ì ‘ ì‹¤í–‰ (ê°œë°œìš©)
```bash
# 1. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /home/ubuntu/llm-fastapi-server

# 2. src ë””ë ‰í† ë¦¬ë¡œ ì´ë™ í›„ ì‹¤í–‰
cd src
python main.py

# ì„œë²„ê°€ ì‹œì‘ë˜ë©´ ë‹¤ìŒ ë©”ì‹œì§€ê°€ í‘œì‹œë©ë‹ˆë‹¤:
# INFO: Starting LLM FastAPI Server...
# INFO: Server is ready!
# INFO: Uvicorn running on http://0.0.0.0:8000
```

#### ë°©ë²• 2: uvicorn ì‚¬ìš© (ê¶Œì¥)
```bash
# 1. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰
cd /home/ubuntu/llm-fastapi-server

# 2. ê°œë°œ ëª¨ë“œ (ìë™ ì¬ì‹œì‘)
uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload

# 3. í”„ë¡œë•ì…˜ ëª¨ë“œ (ì•ˆì •ì )
uvicorn src.main:app --host 0.0.0.0 --port 8000 --workers 4

# 4. íŠ¹ì • ëª¨ë¸ë¡œ ì‹œì‘
MODEL_ID="Qwen/Qwen2.5-7B-Instruct" uvicorn src.main:app --host 0.0.0.0 --port 8000
```

#### ë°©ë²• 3: Docker ì‚¬ìš© (ë°°í¬ìš©)
```bash
# 1. Docker Composeë¡œ ë¹Œë“œ ë° ì‹¤í–‰ (ì¶”ì²œ)
docker-compose up -d

# 2. ê°œë³„ Docker ì‹¤í–‰
docker build -t llm-fastapi-server .
docker run -p 8000:8000 \
  -e HUGGINGFACE_TOKEN="your_token" \
  -v $(pwd)/data:/app/data \
  llm-fastapi-server

# 3. GPU ì§€ì› Docker ì‹¤í–‰
docker run --gpus all -p 8000:8000 \
  -e HUGGINGFACE_TOKEN="your_token" \
  -v $(pwd)/data:/app/data \
  llm-fastapi-server
```

#### ğŸ” ì„œë²„ ìƒíƒœ í™•ì¸

ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë°©ë²•:

```bash
# 1. ê¸°ë³¸ ìƒíƒœ í™•ì¸
curl http://localhost:8000/

# 2. í—¬ìŠ¤ ì²´í¬
curl http://localhost:8000/api/v1/health

# 3. ì§€ì› ëª¨ë¸ ëª©ë¡ í™•ì¸
curl http://localhost:8000/api/v1/models

# 4. ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸
# http://localhost:8000/docs (Swagger UI)
# http://localhost:8000/redoc (ReDoc)
```

#### âš ï¸ ë¬¸ì œ í•´ê²°

**ì„œë²„ ì‹œì‘ ì‹¤íŒ¨ ì‹œ í™•ì¸ì‚¬í•­:**

1. **í¬íŠ¸ ì¶©ëŒ í•´ê²°**
   ```bash
   # 8000 í¬íŠ¸ê°€ ì‚¬ìš© ì¤‘ì¸ ê²½ìš°
   lsof -i :8000
   kill -9 <PID>
   
   # ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
   uvicorn src.main:app --host 0.0.0.0 --port 8001
   ```

2. **Hugging Face í† í° ì˜¤ë¥˜**
   ```bash
   # í† í° í™•ì¸
   echo $HUGGINGFACE_TOKEN
   
   # í† í° ì¬ì„¤ì •
   export HUGGINGFACE_TOKEN="hf_xxxxxxxxxxxxx"
   ```

3. **GPU ë©”ëª¨ë¦¬ ë¶€ì¡±**
   ```bash
   # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
   nvidia-smi
   
   # ë” ì‘ì€ ëª¨ë¸ë¡œ ì‹œì‘
   MODEL_ID="Qwen/Qwen2.5-1.5B-Instruct" python src/main.py
   ```

4. **íŒ¨í‚¤ì§€ ì˜ì¡´ì„± ì˜¤ë¥˜**
   ```bash
   # ì˜ì¡´ì„± ì¬ì„¤ì¹˜
   pip install -r requirements.txt --upgrade
   ```

#### ğŸ¯ ì„œë²„ ì‹œì‘ í›„ ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸

```bash
# 1. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/api/v1/generate" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "ì•ˆë…•í•˜ì„¸ìš”", "max_length": 100}'

# 2. ì±„íŒ… í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/api/v1/chat" \
     -H "Content-Type: application/json" \
     -d '{"message": "ë°˜ê°‘ìŠµë‹ˆë‹¤!"}'

# 3. ëª¨ë¸ ì „í™˜ í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/api/v1/models/switch" \
     -H "Content-Type: application/json" \
     -d '{"model_key": "qwen2.5-7b"}'
```

#### ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```bash
# 1. GPU ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# 2. ì„œë²„ ë¡œê·¸ í™•ì¸
tail -f logs/server.log  # ë¡œê·¸ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°

# 3. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
htop
```

### API ì—”ë“œí¬ì¸íŠ¸

ì„œë²„ê°€ ì‹¤í–‰ë˜ë©´ ëŒ€í™”í˜• API ë¬¸ì„œì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`

#### ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸:

| ì¹´í…Œê³ ë¦¬ | ì—”ë“œí¬ì¸íŠ¸ | ë©”ì†Œë“œ | ì„¤ëª… |
|----------|-----------|--------|------|
| **ê¸°ë³¸ ê¸°ëŠ¥** | `/` | GET | í™˜ì˜ ë©”ì‹œì§€ ë° ì—”ë“œí¬ì¸íŠ¸ ê°œìš” |
| | `/api/v1/health` | GET | ìƒíƒœ í™•ì¸ ë° ì„œë¹„ìŠ¤ ìƒíƒœ |
| | `/api/v1/generate` | POST | LLMì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„± |
| | `/api/v1/chat` | POST | LLMê³¼ ì±„íŒ… |
| | `/api/v1/embed` | POST | í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± |
| | `/api/v1/rag` | POST | RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ |
| **ëª¨ë¸ ê´€ë¦¬** | `/api/v1/models` | GET | ì§€ì›ë˜ëŠ” ëª¨ë“  ëª¨ë¸ ëª©ë¡ ì¡°íšŒ |
| | `/api/v1/models/categories` | GET | ëª¨ë¸ ì¹´í…Œê³ ë¦¬ ë° ë¶„ë¥˜ ì •ë³´ |
| | `/api/v1/models/category/{category}` | GET | íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë¸ë“¤ ì¡°íšŒ |
| | `/api/v1/models/recommend` | POST | ì‹œìŠ¤í…œ ì‚¬ì–‘ ë§ì¶¤ ëª¨ë¸ ì¶”ì²œ |
| | `/api/v1/models/compare` | POST | ì„ íƒëœ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ ë¹„êµ |
| | `/api/v1/models/search` | GET | ëª¨ë¸ ê²€ìƒ‰ ë° í•„í„°ë§ |
| | `/api/v1/models/stats` | GET | ì „ì²´ ëª¨ë¸ í†µê³„ ì •ë³´ |
| | `/api/v1/models/switch` | POST | í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì „í™˜ |
| | `/api/v1/models/info/{model_key}` | GET | íŠ¹ì • ëª¨ë¸ì˜ ìƒì„¸ ì •ë³´ |
| **ì‹œìŠ¤í…œ ì •ë³´** | `/api/v1/system/gpu` | GET | GPU ë©”ëª¨ë¦¬ ë° ì‚¬ìš©ëŸ‰ ì •ë³´ |
| | `/api/v1/models/{model_key}` | GET | íŠ¹ì • ëª¨ë¸ ì •ë³´ ì¡°íšŒ |
| | `/api/v1/models/switch` | POST | ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì „í™˜ |
| **ì‹œìŠ¤í…œ** | `/api/v1/system/gpu` | GET | GPU ìƒíƒœ ë° ë©”ëª¨ë¦¬ ì •ë³´ |

### API í˜¸ì¶œ ì˜ˆì‹œ

#### ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
```bash
curl -X GET "http://localhost:8000/api/v1/models"
```

#### íŠ¹ì • ëª¨ë¸ ì •ë³´ ì¡°íšŒ
```bash
curl -X GET "http://localhost:8000/api/v1/models/qwen2.5-7b"
```

#### ëª¨ë¸ ì „í™˜
```bash
curl -X POST "http://localhost:8000/api/v1/models/switch" \
     -H "Content-Type: application/json" \
     -d '{"model_key": "llama3.1-8b"}'
```

#### íŠ¹ì • ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
```bash
curl -X POST "http://localhost:8000/api/v1/generate" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”", "max_length": 512, "model_key": "solar-10.7b"}'
```

#### í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ë¡œ ì±„íŒ…
```bash
curl -X POST "http://localhost:8000/api/v1/chat" \
     -H "Content-Type: application/json" \
     -d '{"message": "í•œêµ­ì˜ ì „í†µ ìŒì‹ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”", "model_key": "kullm-polyglot-12.8b"}'
```

#### ì½”ë”© íŠ¹í™” ëª¨ë¸ë¡œ ì½”ë“œ ìƒì„±
```bash
curl -X POST "http://localhost:8000/api/v1/generate" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "Pythonìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”", "model_key": "codellama-7b"}'
```

#### RAG ì§ˆì˜ (íŠ¹ì • ëª¨ë¸ ì‚¬ìš©)
```bash
curl -X POST "http://localhost:8000/api/v1/rag" \
     -H "Content-Type: application/json" \
     -d '{"question": "íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ ë¬´ì—‡ì¸ê°€ìš”?", "model_key": "qwen2.5-7b"}'
```

#### ì„ë² ë”© ìƒì„±
```bash
curl -X POST "http://localhost:8000/api/v1/embed" \
     -H "Content-Type: application/json" \
     -d '{"text": "ì„ë² ë”©ì„ ìœ„í•œ ìƒ˜í”Œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤"}'
```

#### GPU ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ
```bash
curl -X GET "http://localhost:8000/api/v1/system/gpu"
```

#### ëª¨ë¸ ê´€ë¦¬ API í˜¸ì¶œ ì˜ˆì‹œ

##### ì§€ì›ë˜ëŠ” ëª¨ë“  ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
```bash
curl -X GET "http://localhost:8000/api/v1/models"
```

##### ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ì¡°íšŒ
```bash
# í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ë“¤
curl -X GET "http://localhost:8000/api/v1/models/category/korean"

# ì½”ë”© íŠ¹í™” ëª¨ë¸ë“¤  
curl -X GET "http://localhost:8000/api/v1/models/category/code"

# ê²½ëŸ‰ ëª¨ë¸ë“¤
curl -X GET "http://localhost:8000/api/v1/models/category/light"
```

##### ì‹œìŠ¤í…œ ì‚¬ì–‘ì— ë§ëŠ” ëª¨ë¸ ì¶”ì²œ
```bash
# 16GB RAM, 8GB GPU í™˜ê²½ì—ì„œ í•œêµ­ì–´ ì‘ì—…ìš© ëª¨ë¸ ì¶”ì²œ
curl -X POST "http://localhost:8000/api/v1/models/recommend" \
     -H "Content-Type: application/json" \
     -d '{"ram_gb": 16, "gpu_gb": 8, "use_case": "korean"}'

# ì½”ë”© ì‘ì—…ìš© ëª¨ë¸ ì¶”ì²œ
curl -X POST "http://localhost:8000/api/v1/models/recommend" \
     -H "Content-Type: application/json" \
     -d '{"ram_gb": 32, "gpu_gb": 16, "use_case": "coding"}'
```

##### ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
```bash
# íŠ¹ì • ëª¨ë¸ë“¤ ë¹„êµ
curl -X POST "http://localhost:8000/api/v1/models/compare" \
     -H "Content-Type: application/json" \
     -d '{"model_keys": ["qwen2.5-7b", "llama3.1-8b", "mistral-7b"]}'

# ëª¨ë“  ëª¨ë¸ ë¹„êµ (model_keys ìƒëµ ì‹œ)
curl -X POST "http://localhost:8000/api/v1/models/compare" \
     -H "Content-Type: application/json" \
     -d '{}'
```

##### ëª¨ë¸ ê²€ìƒ‰ ë° í•„í„°ë§
```bash
# RAM 8-20GB ë²”ìœ„ì˜ ì¤‘í˜• ëª¨ë¸ ê²€ìƒ‰
curl -X GET "http://localhost:8000/api/v1/models/search?category=medium&min_ram=8&max_ram=20"

# "qwen" í‚¤ì›Œë“œë¡œ ëª¨ë¸ ê²€ìƒ‰
curl -X GET "http://localhost:8000/api/v1/models/search?keyword=qwen"

# GPU ë©”ëª¨ë¦¬ 8GB ì´í•˜ ëª¨ë¸ ê²€ìƒ‰
curl -X GET "http://localhost:8000/api/v1/models/search?max_gpu=8"
```

##### ëª¨ë¸ í†µê³„ ì •ë³´
```bash
curl -X GET "http://localhost:8000/api/v1/models/stats"
```

## ğŸ”§ ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜

| ë³€ìˆ˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `HUGGINGFACE_TOKEN` | Hugging Face API í† í° | í•„ìˆ˜ |
| `MODEL_ID` | LLM ëª¨ë¸ ì‹ë³„ì | `Qwen/Qwen2.5-1.5B-Instruct` |
| `EMBEDDING_MODEL` | ì„ë² ë”© ëª¨ë¸ ì´ë¦„ | `BAAI/bge-m3` |
| `MAX_TOKENS` | ìµœëŒ€ ìƒì„± í† í° ìˆ˜ | `512` |
| `TEMPERATURE` | ìƒì„± ì˜¨ë„ | `0.7` |

### ëª¨ë¸ ì„¤ì •

ì„œë²„ëŠ” **40ê°œ ì´ìƒ**ì˜ ë‹¤ì–‘í•œ Hugging Face ëª¨ë¸ì„ ì§€ì›í•©ë‹ˆë‹¤:

#### ğŸ”¥ ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ë¶„ë¥˜

**ğŸª¶ ì´ˆê²½ëŸ‰/ê²½ëŸ‰ ëª¨ë¸ (0.5B-3B)**
- `qwen2.5-0.5b`: Qwen 2.5 0.5B - ì´ˆê²½ëŸ‰ ëª¨ë¸ (2GB RAM, 1GB GPU)
- `qwen2.5-1.5b`: Qwen 2.5 1.5B - í•œêµ­ì–´ ì§€ì› (4GB RAM, 2GB GPU)
- `llama3.2-1b`: Meta Llama 3.2 1B - ê²½ëŸ‰ (4GB RAM, 2GB GPU)  
- `llama3.2-3b`: Meta Llama 3.2 3B - ê· í˜•ì¡íŒ (6GB RAM, 4GB GPU)
- `phi3-mini`: Microsoft Phi-3 Mini 3.8B - ê³ íš¨ìœ¨ (6GB RAM, 3GB GPU)
- `phi3.5-mini`: Microsoft Phi-3.5 Mini 3.8B - ìµœì‹  (6GB RAM, 3GB GPU)
- `gemma2-2b`: Google Gemma 2 2B - ì´ˆê²½ëŸ‰ (4GB RAM, 2GB GPU)

**âš–ï¸ ì¤‘í˜• ëª¨ë¸ (7B-13B)**  
- `qwen2.5-7b`: Qwen 2.5 7B - ê³ ì„±ëŠ¥ (16GB RAM, 8GB GPU)
- `llama3.1-8b`: Meta Llama 3.1 8B - ìµœì‹  (16GB RAM, 8GB GPU)
- `llama3-8b`: Meta Llama 3 8B - ì•ˆì • ë²„ì „ (16GB RAM, 8GB GPU)
- `mistral-7b`: Mistral 7B - ê³ íš¨ìœ¨ (16GB RAM, 8GB GPU)
- `mistral-nemo`: Mistral Nemo 12B - ìµœì‹  (24GB RAM, 12GB GPU)
- `gemma2-9b`: Google Gemma 2 9B (18GB RAM, 10GB GPU)
- `yi-9b`: 01.AI Yi 1.5 9B - ë‹¤êµ­ì–´ (18GB RAM, 10GB GPU)
- `vicuna-7b`: LMSYS Vicuna 7B - ëŒ€í™”í˜• (16GB RAM, 8GB GPU)

**ğŸ‡°ğŸ‡· í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸**
- `solar-10.7b`: Upstage SOLAR 10.7B - í•œêµ­ì–´ íŠ¹í™” (20GB RAM, 12GB GPU)
- `kullm-polyglot-12.8b`: KULLM Polyglot 12.8B - í•œêµ­ì–´ (24GB RAM, 14GB GPU)
- `ko-alpaca`: Beomi KoAlpaca 5.8B - í•œêµ­ì–´ (12GB RAM, 6GB GPU)
- `eeve-korean-10.8b`: Yanolja EEVE 10.8B - í•œêµ­ì–´ íŠ¹í™” (20GB RAM, 12GB GPU)

**ğŸ’» ì½”ë“œ íŠ¹í™” ëª¨ë¸**
- `codellama-7b`: Meta Code Llama 7B - ì½”ë”© íŠ¹í™” (16GB RAM, 8GB GPU)
- `codellama-13b`: Meta Code Llama 13B - ê³ ê¸‰ ì½”ë”© (26GB RAM, 16GB GPU)
- `deepseek-coder-6.7b`: DeepSeek Coder 6.7B (14GB RAM, 7GB GPU)
- `deepseek-coder-33b`: DeepSeek Coder 33B - ê³ ê¸‰ (66GB RAM, 33GB GPU)
- `starcoder2-7b`: BigCode StarCoder2 7B (16GB RAM, 8GB GPU)
- `codeqwen-7b`: Qwen CodeQwen 7B - ì½”ë”© íŠ¹í™” (16GB RAM, 8GB GPU)

**ğŸ§® ìˆ˜í•™/ê³¼í•™ íŠ¹í™” ëª¨ë¸**
- `mathstral-7b`: Mistral Mathstral 7B - ìˆ˜í•™ íŠ¹í™” (16GB RAM, 8GB GPU)
- `deepseek-math-7b`: DeepSeek Math 7B - ìˆ˜í•™ íŠ¹í™” (16GB RAM, 8GB GPU)

**ğŸš€ ëŒ€í˜• ëª¨ë¸ (14B+)**
- `qwen2.5-14b`: Qwen 2.5 14B - ê³ ì„±ëŠ¥ (28GB RAM, 16GB GPU)
- `qwen2.5-32b`: Qwen 2.5 32B - ëŒ€í˜• (64GB RAM, 32GB GPU)  
- `qwen2.5-72b`: Qwen 2.5 72B - ìµœê³  ì„±ëŠ¥ (144GB RAM, 72GB GPU)
- `llama3.1-70b`: Meta Llama 3.1 70B - ìµœê³  ì„±ëŠ¥ (140GB RAM, 70GB GPU)
- `mixtral-8x7b`: Mistral Mixtral 8x7B MoE (90GB RAM, 45GB GPU)

**ğŸŒ ë‹¤êµ­ì–´ íŠ¹í™” ëª¨ë¸**
- `aya-23-8b`: Cohere Aya 23 8B - ë‹¤êµ­ì–´ (16GB RAM, 8GB GPU)
- `bloom-7b`: BigScience BLOOM 7B - ë‹¤êµ­ì–´ (16GB RAM, 8GB GPU)

#### ğŸ“‹ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

**ìš©ë„ë³„ ì¶”ì²œ ëª¨ë¸:**
- **ì¼ë°˜ ëŒ€í™”/í…ìŠ¤íŠ¸ ìƒì„±**: `qwen2.5-1.5b`, `llama3.2-3b`, `qwen2.5-7b`
- **í•œêµ­ì–´ íŠ¹í™” ì‘ì—…**: `solar-10.7b`, `kullm-polyglot-12.8b`, `eeve-korean-10.8b`
- **í”„ë¡œê·¸ë˜ë°/ì½”ë”©**: `codellama-7b`, `deepseek-coder-6.7b`, `codeqwen-7b`
- **ìˆ˜í•™/ê³¼í•™**: `mathstral-7b`, `deepseek-math-7b`
- **ë‹¤êµ­ì–´ ì§€ì›**: `aya-23-8b`, `bloom-7b`, `yi-9b`
- **ìµœê³  ì„±ëŠ¥ (ë¦¬ì†ŒìŠ¤ ì¶©ë¶„)**: `qwen2.5-72b`, `llama3.1-70b`, `mixtral-8x7b`

**ì‹œìŠ¤í…œ ì‚¬ì–‘ë³„ ì¶”ì²œ:**
- **4GB RAM, 2GB GPU**: `qwen2.5-0.5b`, `qwen2.5-1.5b`, `gemma2-2b`
- **8GB RAM, 4GB GPU**: `llama3.2-3b`, `phi3-mini`, `phi3.5-mini`
- **16GB RAM, 8GB GPU**: `qwen2.5-7b`, `llama3.1-8b`, `mistral-7b`, `codellama-7b`
- **32GB+ RAM, 16GB+ GPU**: `qwen2.5-14b`, `solar-10.7b`, `codellama-13b`
- **solar-10.7b**: `upstage/SOLAR-10.7B-Instruct-v1.0` - Upstageì˜ í•œêµ­ì–´ íŠ¹í™”
- **kullm-polyglot-12.8b**: `nlpai-lab/kullm-polyglot-12.8b-v2` - í•œêµ­ì–´ ì „ìš©

#### ğŸ’» ì½”ë“œ íŠ¹í™” ëª¨ë¸
- **codellama-7b**: `codellama/CodeLlama-7b-Instruct-hf` - Metaì˜ ì½”ë”© íŠ¹í™”
- **deepseek-coder-6.7b**: `deepseek-ai/deepseek-coder-6.7b-instruct` - ì½”ë”© ì „ë¬¸

#### ğŸš€ ëŒ€í˜• ëª¨ë¸ (30B+) - ìµœê³  ì„±ëŠ¥
- **qwen2.5-32b**: `Qwen/Qwen2.5-32B-Instruct` - ëŒ€í˜• ê³ ì„±ëŠ¥
- **llama3.1-70b**: `meta-llama/Meta-Llama-3.1-70B-Instruct` - ìµœê³  ì„±ëŠ¥ (ë©€í‹° GPU í•„ìš”)

## ğŸ§ª ê°œë°œ

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
# ê°œë°œ ë…¸íŠ¸ë¶ ì‹¤í–‰
jupyter lab notebooks/
```

### ìƒˆ ëª¨ë¸ ì¶”ê°€

1. ìƒˆ ëª¨ë¸ ì„¤ì •ìœ¼ë¡œ `llm_handler.py` ì—…ë°ì´íŠ¸
2. ìƒˆ ì˜ì¡´ì„±ì´ í•„ìš”í•œ ê²½ìš° `requirements.txt` ìˆ˜ì •
3. ì œê³µëœ ë…¸íŠ¸ë¶ìœ¼ë¡œ í…ŒìŠ¤íŠ¸

## ğŸ“¦ Docker ë°°í¬

### ì´ë¯¸ì§€ ë¹Œë“œ
```bash
docker build -t llm-fastapi-server .
```

### ì»¨í…Œì´ë„ˆ ì‹¤í–‰
```bash
docker run -p 8000:8000 \
  -e HUGGINGFACE_TOKEN="your_token" \
  -v $(pwd)/data:/app/data \
  llm-fastapi-server
```

### í”„ë¡œë•ì…˜ ë°°í¬
```bash
docker-compose up -d
```

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. ì €ì¥ì†Œ í¬í¬
2. ê¸°ëŠ¥ ë¸Œëœì¹˜ ìƒì„± (`git checkout -b feature/amazing-feature`)
3. ë³€ê²½ì‚¬í•­ ì»¤ë°‹ (`git commit -m 'Add amazing feature'`)
4. ë¸Œëœì¹˜ì— í‘¸ì‹œ (`git push origin feature/amazing-feature`)
5. Pull Request ì—´ê¸°

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë¼ì´ì„ ìŠ¤ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤ - ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ™ ê°ì‚¬ì˜ ë§

- ìš°ìˆ˜í•œ ëª¨ë¸ í˜¸ìŠ¤íŒ…ì„ ì œê³µí•˜ëŠ” [Hugging Face](https://huggingface.co/)
- ë†€ë¼ìš´ ì›¹ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•˜ëŠ” [FastAPI](https://fastapi.tiangolo.com/)
- RAG ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” [LangChain](https://langchain.com/)
- ì„ë² ë”© ëª¨ë¸ì„ ì œê³µí•˜ëŠ” [Sentence Transformers](https://www.sbert.net/)

## ğŸ“ ì§€ì›

ì§ˆë¬¸ ë° ì§€ì›:

- ì´ ì €ì¥ì†Œì—ì„œ ì´ìŠˆ ìƒì„±
- ì„œë²„ ì‹¤í–‰ ì‹œ [ë¬¸ì„œ](http://localhost:8000/docs) í™•ì¸
- `notebooks/` ë””ë ‰í† ë¦¬ì˜ ì˜ˆì œ ë…¸íŠ¸ë¶ ê²€í† 

---

**ì°¸ê³ **: ì´ ì„œë²„ëŠ” êµìœ¡ ë° ê°œë°œ ëª©ì ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. í”„ë¡œë•ì…˜ ë°°í¬ì˜ ê²½ìš° ì ì ˆí•œ ë³´ì•ˆ ì¡°ì¹˜, ì¸ì¦ ë° í™•ì¥ êµ¬ì„±ì„ ë³´ì¥í•˜ì„¸ìš”.